{"componentChunkName":"component---node-modules-rocketseat-gatsby-theme-docs-core-src-templates-docs-query-js","path":"/conclusions/","result":{"data":{"mdx":{"id":"4986f214-e4ff-505b-b91a-d465f216a005","excerpt":"We have applied multiple intervention strategies to two different data sets, we gather here some over-arching thoughts based on our experiences. Interventionsâ€¦","fields":{"slug":"/conclusions/"},"frontmatter":{"title":"Conclusions","description":null,"image":null,"disableTableOfContents":null},"body":"function _extends() { _extends = Object.assign || function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\n\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n\n/* @jsx mdx */\nvar _frontmatter = {\n  \"title\": \"Conclusions\"\n};\n\nvar makeShortcode = function makeShortcode(name) {\n  return function MDXDefaultShortcode(props) {\n    console.warn(\"Component \" + name + \" was not imported, exported, or provided by MDXProvider as global scope\");\n    return mdx(\"div\", props);\n  };\n};\n\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n      props = _objectWithoutProperties(_ref, [\"components\"]);\n\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"p\", null, \"We have applied multiple intervention strategies to two different data sets, we gather here some over-arching thoughts based on our experiences.\"), mdx(\"h2\", {\n    \"id\": \"interventions\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, mdx(\"a\", _extends({\n    parentName: \"h2\"\n  }, {\n    \"href\": \"#interventions\",\n    \"aria-label\": \"interventions permalink\",\n    \"className\": \"anchor before\"\n  }), mdx(\"svg\", _extends({\n    parentName: \"a\"\n  }, {\n    \"aria-hidden\": \"true\",\n    \"focusable\": \"false\",\n    \"height\": \"16\",\n    \"version\": \"1.1\",\n    \"viewBox\": \"0 0 16 16\",\n    \"width\": \"16\"\n  }), mdx(\"path\", _extends({\n    parentName: \"svg\"\n  }, {\n    \"fillRule\": \"evenodd\",\n    \"d\": \"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"\n  })))), \"Interventions\"), mdx(\"p\", null, \"It's clear that not all interventions are created equally. In part this is just because some of the interventions we tried are old and have been superceded with more effective approaches.\"), mdx(\"p\", null, \"Overall post-processing techniques tend to be the easiest to apply, as they do not impose restrictions on the type of model that can be used, and usually aren't very computationally intensive. For demographic parity the reject option classification method of \", mdx(\"a\", _extends({\n    parentName: \"p\"\n  }, {\n    \"href\": \"/cdei-development/interventions#label-modification---kamiran-et-al\"\n  }), \"Kamrian et al.\"), \" proved effective. For equalised odds the post-processing intervention of \", mdx(\"a\", _extends({\n    parentName: \"p\"\n  }, {\n    \"href\": \"/cdei-development/interventions#decision-threshold-modification---hardt-et-al\"\n  }), \"Hardt et al.\"), \" proved very effective.\"), mdx(\"p\", null, \"Post-processing techniques are attractive for their simplicity, but for optimal performance we find that in-processing techniques had the potential for better performance, though sometimes could be tricky to tune. For demographic parity and conditional demographic parity the adversarial debiasing approach of \", mdx(\"a\", _extends({\n    parentName: \"p\"\n  }, {\n    \"href\": \"/cdei-development/interventions#adversarial-debiasing---zhang-et-al\"\n  }), \"Zhang et al.\"), \" proved extremely effective. We were not able to get it to perform so well for equalised odds, though it's unclear if more careful tuning could overcome the difficulties we experienced with our simple implementation. For equalised odds and equal opportunity the reductions approach of \", mdx(\"a\", _extends({\n    parentName: \"p\"\n  }, {\n    \"href\": \"/cdei-development/interventions/#reductions-approach-via-constrained-optimisation---agarwal-et-al\"\n  }), \"Agarwal et al.\"), \" was very effective, but did not match the performance of adversarial debiasing for demographic parity.\"), mdx(\"p\", null, \"Based on our experiences, pre-processing interventions could be used as a step in an overall pipeline, but they did not by themselves do enough to mitigate bias.\"), mdx(\"h2\", {\n    \"id\": \"tooling\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, mdx(\"a\", _extends({\n    parentName: \"h2\"\n  }, {\n    \"href\": \"#tooling\",\n    \"aria-label\": \"tooling permalink\",\n    \"className\": \"anchor before\"\n  }), mdx(\"svg\", _extends({\n    parentName: \"a\"\n  }, {\n    \"aria-hidden\": \"true\",\n    \"focusable\": \"false\",\n    \"height\": \"16\",\n    \"version\": \"1.1\",\n    \"viewBox\": \"0 0 16 16\",\n    \"width\": \"16\"\n  }), mdx(\"path\", _extends({\n    parentName: \"svg\"\n  }, {\n    \"fillRule\": \"evenodd\",\n    \"d\": \"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"\n  })))), \"Tooling\"), mdx(\"p\", null, \"There are a large number of open-source fairness libraries available, however many of them can only be used to measure bias in models, there are relatively few that actually implement any of these intervention strategies. The two most notable are \", mdx(\"a\", _extends({\n    parentName: \"p\"\n  }, {\n    \"href\": \"https://aif360.readthedocs.io/en/latest/\"\n  }), \"AI Fairness 360\"), \" from IBM and \", mdx(\"a\", _extends({\n    parentName: \"p\"\n  }, {\n    \"href\": \"https://fairlearn.github.io/\"\n  }), \"Fairlearn\"), \" from Microsoft. Our opinion is that Fairlearn is better curated, more robustly engineered, and has a more flexible interface and hence would be our recommendation if you had to pick one. But both libraries offer a lot of functionality.\"), mdx(\"h3\", {\n    \"id\": \"ai-fairness-360\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, mdx(\"a\", _extends({\n    parentName: \"h3\"\n  }, {\n    \"href\": \"#ai-fairness-360\",\n    \"aria-label\": \"ai fairness 360 permalink\",\n    \"className\": \"anchor before\"\n  }), mdx(\"svg\", _extends({\n    parentName: \"a\"\n  }, {\n    \"aria-hidden\": \"true\",\n    \"focusable\": \"false\",\n    \"height\": \"16\",\n    \"version\": \"1.1\",\n    \"viewBox\": \"0 0 16 16\",\n    \"width\": \"16\"\n  }), mdx(\"path\", _extends({\n    parentName: \"svg\"\n  }, {\n    \"fillRule\": \"evenodd\",\n    \"d\": \"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"\n  })))), \"AI Fairness 360\"), mdx(\"p\", null, \"This is easily the more comprehensive of the two offerings. At the time of writing they have implemented 10 different bias mitigation algorithms covering pre-processing, in-processing and post-processing techniques. Several of these implementation are ported from implementations that accompanied the original research papers, and as such the library offers a \\\"one-stop shop\\\" for fairness algorithms with a unified interface for each algorithm.\"), mdx(\"p\", null, \"There is perhaps a bit of a danger that arises from the comprehensive nature of the library which is that its creators seem relatively unopinionated about which interventions are the best and should be favoured over others. Additionally, because the implementations have been ported from research code, in some cases with only minor modification, in our opinion not all of the interventions are suitable for production environments.\"), mdx(\"p\", null, \"Furthermore, we found the interface to be a little awkward, requiring data to be passed to algorithms as custom data set types that they define. A recent release has updated the API to be scikit-learn compatible, so hopefully usability will continue to improve over time.\"), mdx(\"h3\", {\n    \"id\": \"fairlearn\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, mdx(\"a\", _extends({\n    parentName: \"h3\"\n  }, {\n    \"href\": \"#fairlearn\",\n    \"aria-label\": \"fairlearn permalink\",\n    \"className\": \"anchor before\"\n  }), mdx(\"svg\", _extends({\n    parentName: \"a\"\n  }, {\n    \"aria-hidden\": \"true\",\n    \"focusable\": \"false\",\n    \"height\": \"16\",\n    \"version\": \"1.1\",\n    \"viewBox\": \"0 0 16 16\",\n    \"width\": \"16\"\n  }), mdx(\"path\", _extends({\n    parentName: \"svg\"\n  }, {\n    \"fillRule\": \"evenodd\",\n    \"d\": \"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"\n  })))), \"Fairlearn\"), mdx(\"p\", null, \"Fairlearn is a more recent library, having only been under active development for the last year or so. It currently has less functionality than IBM's offering, though seems to be more actively developed so may catch up.\"), mdx(\"p\", null, \"At the moment there are only three mitigation algorithms implemented, that of Hardt et al., Agarwal et al. and a grid search approach that we didn't consider in this work. In our opinion the interface is easier to integrate into an existing workflow, the quality of the code is more consistent, and the algorithms available are better curated - in our experiments the approaches of Hardt et al. and Agarwal et al. both performed extremely well, whereas some of the approaches implemented by AI Fairness 360 were quite poor.\"), mdx(\"p\", null, \"Fairlearn also includes an interactive dashboard for model exploration that can help assess and understand unfairness in your model. Given the need for contextual understanding in effectively mitigating bias this could prove a valuable component of a bias mitigation workflow.\"));\n}\n;\nMDXContent.isMDXComponent = true;","headings":[{"depth":2,"value":"Interventions"},{"depth":2,"value":"Tooling"},{"depth":3,"value":"AI Fairness 360"},{"depth":3,"value":"Fairlearn"}]}},"pageContext":{"slug":"/conclusions/","prev":{"label":"Interventions","link":"/interventions"},"next":{"label":"Additional Resources","link":"/additional-resources"}}}}