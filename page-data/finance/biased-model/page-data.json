{"componentChunkName":"component---src-templates-post-js","path":"/finance/biased-model/","result":{"data":{"mdx":{"body":"function _extends() { _extends = Object.assign || function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\n\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n\n/* @jsx mdx */\nvar _frontmatter = {\n  \"title\": \"Training a biased model\",\n  \"order\": 3\n};\n\nvar makeShortcode = function makeShortcode(name) {\n  return function MDXDefaultShortcode(props) {\n    console.warn(\"Component \" + name + \" was not imported, exported, or provided by MDXProvider as global scope\");\n    return mdx(\"div\", props);\n  };\n};\n\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n      props = _objectWithoutProperties(_ref, [\"components\"]);\n\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"p\", null, \"We start by training a simple model on the data, specifically a shallow, feed-forward neural network. You can see the details here \", mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"ADD NOTEBOOK LINK\"), \". To understand the possible sources of bias in our model, it's useful to first train a model with no attempted mitigation, and understand the resulting unfairness.\"), mdx(\"h2\", null, \"Demographic Parity\"), mdx(\"p\", null, \"Demographic parity requires that we treat all demographic groups equally. We start by investigating the disparity between the sexes. We do this with box plots of the model scores. A higher score means the model thinks the individual is more likely to be earning more than $50,000. It's clear that there is a major disparity between men and women, with men being awarded systematically higher scores. This model therefore does not achieve demographic parity by some margin.\"), mdx(LazyPlot, _extends({}, dp_by_sex, {\n    mdxType: \"LazyPlot\"\n  })), mdx(\"p\", null, \"We do the same thing for race. Again we see that there is a major disparity between races, with white and asian individuals systematically receiving higher scores, and black and native Americans receiving much lower scores.\"), mdx(LazyPlot, _extends({}, dp_by_race, {\n    mdxType: \"LazyPlot\"\n  })), mdx(\"h2\", null, \"Conditional Demographic Parity\"), mdx(\"p\", null, \"Conditional demographic parity requires that all demographic groups are treated equally, once certain legitimate risk factors are taken into account. We will take hours worked per week as the legitimate risk factor. We bucket individuals according to how many hours per week they work, and compare the distribution of scores within those buckets first between the two sexes. As we can see in each bucket women receive lower scores than the men. In other words, the model believes that women who work the same number of hours as men are less likely to be high-earners.\"), mdx(LazyPlot, _extends({}, cdp_by_sex, {\n    mdxType: \"LazyPlot\"\n  })), mdx(\"p\", null, \"We repeat this, grouping individuals by race, and again see that the disadvantaged demographics are less likely to be high-earners, even if they work the same number of hours.\"), mdx(LazyPlot, _extends({}, cdp_by_race, {\n    mdxType: \"LazyPlot\"\n  })), mdx(\"h2\", null, \"Equalised Odds\"), mdx(\"p\", null, \"Equalised odds requires that we treat groups equally once outcomes are taken into account. That is to say, among individuals who do in fact earn more than $50,000, the model predictions are similar or the same for all demographic groups. Likewise among individuals who don't earn more than $50,000, the model treats different groups equally.\"), mdx(\"p\", null, \"This is equivalent to requiring equal true and false positive rates on different groups, and so we can measure to what extent we are achieving equalised odds by comparing the \", mdx(\"a\", _extends({\n    parentName: \"p\"\n  }, {\n    \"href\": \"https://en.wikipedia.org/wiki/Receiver_operating_characteristic\"\n  }), \"ROC curves\"), \". Doing this first for the two sexes, we see that there is a large disparity, but it is actually women who receive better classification accuracy, that is the false positive rate is lower and the true positive rate is higher.\"), mdx(LazyPlot, _extends({}, eo_by_sex, {\n    mdxType: \"LazyPlot\"\n  })), mdx(\"p\", null, \"We see a similar picture when comparing the different races. The races other than white and black aren't very well represented in the data, which means the ROC curves are not very smooth and hence not very useful. But comparing just white and black we see that the true positive rate is higher and the false positive rate lower on average for black individuals.\"), mdx(LazyPlot, _extends({}, eo_by_race, {\n    mdxType: \"LazyPlot\"\n  })), mdx(\"p\", null, \"These observations are in contrast to the previously observed demographic disparities. We observed for example that women systematically are given lower scores, which we could argue disadvantages women. On the other hand, we have just observed that classification accuracy is better for women, they are less likely to be misclassified, which we could argue \", mdx(\"em\", {\n    parentName: \"p\"\n  }, \"advantages\"), \" women. Specifically, the higher classification accuracy means they are less likely to be marketted or granted a credit card or a loan which they cannot afford, and hence less likely to default on that credit card or loan and have to deal with the resulting negative consequences. This observation once again underlines the need for understanding the context of the problem, and making a careful determination of what unfairness actually means.\"));\n}\n;\nMDXContent.isMDXComponent = true;","frontmatter":{"title":"Training a biased model"},"fields":{"collection":"finance"}}},"pageContext":{"slug":"/finance/biased-model/","next":null,"previous":{"fields":{"collection":"finance","slug":"/finance/data/"},"frontmatter":{"order":2,"title":"The Adult data set"}}}}}