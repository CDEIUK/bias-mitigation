{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fairness Through Unawareness - Recruiting data\n",
    "\n",
    "This notebook contains the implementation of the common pre-processing intervention called Fairness Through Unawareness (FTU) in which the protected attribute is not included as a feature in the training data. Besides being considered as an intervention, FTU can also be considered as a fairness notion, which is consistent with disparate treatment.\n",
    "\n",
    "Although FTU is often applied by industry practitioners, its effect in terms of reducing unfairness is limited since information on protected attributed can still be contained elsewhere in the data. More precisely, there may be features which are highly correlated with the protected attributes and therefore act as proxies for them.\n",
    "\n",
    "We consider the effect of applying FTU for a number of observational group fairness notions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from fairlearn.metrics import (\n",
    "    demographic_parity_difference,\n",
    "    equalized_odds_difference,\n",
    ")\n",
    "from sklearn.neural_network import MLPClassifier  # noqa\n",
    "from helpers.metrics import accuracy\n",
    "from helpers.plot import group_bar_plots, group_box_plots, calibration_curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n",
    "We have committed preprocessed data to the repository for reproducibility and we load it here. Check out hte preprocessing notebook for details on how this data was obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artifacts_dir = Path(\"../../../artifacts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = artifacts_dir / \"data\" / \"recruiting\"\n",
    "\n",
    "train = pd.read_csv(data_dir / \"processed\" / \"train.csv\")\n",
    "val = pd.read_csv(data_dir / \"processed\" / \"val.csv\")\n",
    "test = pd.read_csv(data_dir / \"processed\" / \"test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load original model\n",
    "\n",
    "For maximum reproducibility we can also load the baseline model from disk, but the code used to train can be found in the baseline model notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model = joblib.load(\n",
    "    artifacts_dir / \"models\" / \"recruiting\" / \"baseline.pkl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get predictions on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bl_test_probs = baseline_model.predict_proba(\n",
    "    test.drop(\"employed_yes\", axis=1)\n",
    ")[:, 1]\n",
    "bl_test_labels = (bl_test_probs > 0.5).astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn model under FTU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate FTU data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ftu = train.drop(\"race_white\", axis=1).copy()\n",
    "val_ftu = val.drop(\"race_white\", axis=1).copy()\n",
    "test_ftu = test.drop(\"race_white\", axis=1).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learn model on FTU training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ftu_model = MLPClassifier(hidden_layer_sizes=(100, 100), early_stopping=True)\n",
    "# ftu_model.fit(train_ftu.drop(columns=\"employed_yes\"), train.employed_yes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ftu_model = joblib.load(artifacts_dir / \"models\" / \"recruiting\" / \"ftu.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate prediction via learnt FTU model on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_probs = ftu_model.predict_proba(test_ftu.drop(\"employed_yes\", axis=1))[\n",
    "    :, 1\n",
    "]\n",
    "test_pred_labels = (test_probs > 0.5).astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demographic parity\n",
    "\n",
    "We first address the effect on demographic parity using FTU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_race_white = test.race_white.values\n",
    "test_employed_yes = test.employed_yes.values\n",
    "mask = test_race_white == 1\n",
    "\n",
    "# baseline metrics\n",
    "bl_test_acc = accuracy(test_employed_yes, bl_test_probs)\n",
    "bl_test_dpd = demographic_parity_difference(\n",
    "    test.employed_yes, bl_test_labels, sensitive_features=test_race_white,\n",
    ")\n",
    "\n",
    "# new model metrics\n",
    "test_acc = accuracy(test_employed_yes, test_probs)\n",
    "test_dpd = demographic_parity_difference(\n",
    "    test.employed_yes, test_pred_labels, sensitive_features=test_race_white,\n",
    ")\n",
    "\n",
    "print(f\"Baseline accuracy: {bl_test_acc:.3f}\")\n",
    "print(f\"Accuracy: {test_acc:.3f}\\n\")\n",
    "\n",
    "print(f\"Baseline demographic parity: {bl_test_dpd:.3f}\")\n",
    "print(f\"Demographic parity: {test_dpd:.3f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp_box = group_box_plots(\n",
    "    np.concatenate([bl_test_probs, test_probs]),\n",
    "    np.tile(test.race_white.map({0: \"Black\", 1: \"White\"}), 2),\n",
    "    groups=np.concatenate(\n",
    "        [np.zeros_like(bl_test_probs), np.ones_like(test_probs)]\n",
    "    ),\n",
    "    group_names=[\"Baseline\", \"Kamishima\"],\n",
    "    title=\"Distribution of scores by race\",\n",
    "    xlabel=\"Scores\",\n",
    "    ylabel=\"Method\",\n",
    ")\n",
    "dp_box"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Equalised odds\n",
    "\n",
    "Let us now evaluate equalised odds for the FTU model on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_race = test.race_white.values\n",
    "test_employed = test.employed_yes.values\n",
    "mask = test_race == 1\n",
    "\n",
    "# baseline metrics\n",
    "bl_test_acc = accuracy(test_employed, bl_test_probs)\n",
    "bl_test_eod = equalized_odds_difference(\n",
    "    test_employed, bl_test_labels, sensitive_features=test_race,\n",
    ")\n",
    "\n",
    "# new model metrics\n",
    "test_acc = accuracy(test_employed, test_pred_labels)\n",
    "test_eod = equalized_odds_difference(\n",
    "    test_employed, test_pred_labels, sensitive_features=test_race,\n",
    ")\n",
    "\n",
    "print(f\"Baseline accuracy: {bl_test_acc:.3f}\")\n",
    "print(f\"Accuracy: {test_acc:.3f}\\n\")\n",
    "\n",
    "print(f\"Baseline equalised odds (dist.): {bl_test_eod:.3f}\")\n",
    "print(f\"Equalised odds (dist.): {test_eod:.3f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bl_eo_bar = group_bar_plots(\n",
    "    bl_test_probs,\n",
    "    test.race_white.map({0: \"Black\", 1: \"White\"}),\n",
    "    groups=test.employed_yes,\n",
    "    group_names=[\"Not employed\", \"Employed\"],\n",
    "    title=\"Predictions by race and outcome\",\n",
    "    xlabel=\"Proportion predicted successful\",\n",
    "    ylabel=\"Outcome\",\n",
    ")\n",
    "bl_eo_bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eo_bar = group_bar_plots(\n",
    "    test_pred_labels,\n",
    "    test.race_white.map({0: \"Black\", 1: \"White\"}),\n",
    "    groups=test.employed_yes,\n",
    "    group_names=[\"Not employed\", \"Employed\"],\n",
    "    title=\"Predictions by race and outcome\",\n",
    "    xlabel=\"Proportion predicted successful\",\n",
    "    ylabel=\"Outcome\",\n",
    ")\n",
    "eo_bar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Equal opportunity\n",
    "\n",
    "Let us now evaluate equal opportunity for the FTU model on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_race = test.race_white.values\n",
    "test_employed = test.employed_yes.values\n",
    "mask = test_race == 1\n",
    "\n",
    "# baseline metrics\n",
    "bl_test_acc = accuracy(test_employed, bl_test_probs)\n",
    "bl_test_eoppd = equalized_odds_difference(\n",
    "    test_employed[test.employed_yes == 1],\n",
    "    bl_test_labels[test.employed_yes == 1],\n",
    "    sensitive_features=test_race[test.employed_yes == 1],\n",
    ")\n",
    "\n",
    "# new model metrics\n",
    "test_acc = accuracy(test_employed, test_pred_labels)\n",
    "test_eoppd = equalized_odds_difference(\n",
    "    test_employed[test.employed_yes == 1],\n",
    "    test_pred_labels[test.employed_yes == 1],\n",
    "    sensitive_features=test_race[test.employed_yes == 1],\n",
    ")\n",
    "\n",
    "print(f\"Baseline accuracy: {bl_test_acc:.3f}\")\n",
    "print(f\"Accuracy: {test_acc:.3f}\\n\")\n",
    "\n",
    "print(f\"Baseline equal opportunity: {bl_test_eoppd:.3f}\")\n",
    "print(f\"Equal opportunity: {test_eoppd:.3f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = test.employed_yes == 1\n",
    "\n",
    "eopp_bar = group_bar_plots(\n",
    "    np.concatenate([bl_test_labels[mask], test_pred_labels[mask]]),\n",
    "    np.tile(test.race_white[mask].map({0: \"Black\", 1: \"White\"}), 2),\n",
    "    groups=np.concatenate(\n",
    "        [np.zeros_like(bl_test_probs[mask]), np.ones_like(test_probs[mask])]\n",
    "    ),\n",
    "    group_names=[\"Baseline\", \"FTU\"],\n",
    "    title=\"Mean prediction for being employed by race\",\n",
    "    xlabel=\"Proportion predicted being employed\",\n",
    "    ylabel=\"Method\",\n",
    ")\n",
    "eopp_bar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calibration\n",
    "\n",
    "Let us now evaluate calibration for the FTU model on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calibration_curves(\n",
    "    test.employed_yes,\n",
    "    bl_test_probs,\n",
    "    test.race_white.map({0: \"Black\", 1: \"White\"}),\n",
    "    title=\"Baseline calibration by race\",\n",
    "    xlabel=\"Score\",\n",
    "    ylabel=\"Proportion positive outcome\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calibration_curves(\n",
    "    test.employed_yes,\n",
    "    test_probs,\n",
    "    test.race_white.map({0: \"Black\", 1: \"White\"}),\n",
    "    title=\"FTU calibration by race\",\n",
    "    xlabel=\"Score\",\n",
    "    ylabel=\"Proportion positive outcome\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Python3]",
   "language": "python",
   "name": "conda-env-Python3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
