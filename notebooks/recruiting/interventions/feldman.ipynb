{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Certifying and Removing Disparate Impact\n",
    "\n",
    "This notebook apples the algorithm described in [Certifying and removing disparate impact](https://dl.acm.org/doi/10.1145/2783258.2783311) by Feldman et al., as implemented by the [AI Fairness 360 library](https://aif360.readthedocs.io/) from IBM.\n",
    "\n",
    "This is a pre-processing algorithm that works by adjusting the distributions of the features conditional on the protected attribute to be equal, so that a subsequently trained model can't discriminate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from aif360.datasets import StandardDataset\n",
    "from aif360.algorithms.preprocessing import DisparateImpactRemover\n",
    "from helpers.fairness_measures import (\n",
    "    accuracy,\n",
    "    disparate_impact_d,\n",
    "    disparate_impact_p,\n",
    ")\n",
    "from helpers.plot import group_box_plots\n",
    "from sklearn.neural_network import MLPClassifier  # noqa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n",
    "We have committed preprocessed data to the repository for reproducibility and we load it here. Check out the preprocessing notebook for details on how this data was obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artifacts_dir = Path(\"../../../artifacts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = artifacts_dir / \"data\" / \"recruiting\"\n",
    "\n",
    "train = pd.read_csv(data_dir / \"processed\" / \"train.csv\")\n",
    "val = pd.read_csv(data_dir / \"processed\" / \"val.csv\")\n",
    "test = pd.read_csv(data_dir / \"processed\" / \"test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`aif360` uses the following custom dataset objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sds = StandardDataset(\n",
    "    train,\n",
    "    label_name=\"employed_yes\",\n",
    "    favorable_classes=[1],\n",
    "    protected_attribute_names=[\"race_white\"],\n",
    "    privileged_classes=[[1]],\n",
    ")\n",
    "test_sds = StandardDataset(\n",
    "    test,\n",
    "    label_name=\"employed_yes\",\n",
    "    favorable_classes=[1],\n",
    "    protected_attribute_names=[\"race_white\"],\n",
    "    privileged_classes=[[1]],\n",
    ")\n",
    "val_sds = StandardDataset(\n",
    "    val,\n",
    "    label_name=\"employed_yes\",\n",
    "    favorable_classes=[1],\n",
    "    protected_attribute_names=[\"race_white\"],\n",
    "    privileged_classes=[[1]],\n",
    ")\n",
    "index = train_sds.feature_names.index(\"race_white\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train unfair model\n",
    "\n",
    "For maximum reproducibility we load the baseline model from disk, but the code used to train can be found in the baseline model notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bl_model = joblib.load(\n",
    "    artifacts_dir / \"models\" / \"recruiting\" / \"baseline.pkl\"\n",
    ")\n",
    "\n",
    "bl_test_probs = bl_model.predict_proba(test_sds.features)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform intervention\n",
    "\n",
    "We repair the dataset using the `DisparateImpactRemover`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "di = DisparateImpactRemover(repair_level=1.0)\n",
    "\n",
    "train_repd = di.fit_transform(train_sds)\n",
    "train_repd_X = np.delete(train_repd.features, index, axis=1)\n",
    "train_repd_y = train_repd.labels.flatten()\n",
    "\n",
    "test_repd = di.fit_transform(test_sds)\n",
    "test_repd_X = np.delete(test_repd.features, index, axis=1)\n",
    "test_repd_y = test_repd.labels.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model on fair data\n",
    "\n",
    "We use the same architecture, but the repaired data. Once again we load a trained model for reproducibility, but the code used to train the model can be found below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = joblib.load(artifacts_dir / \"models\" / \"recruiting\" / \"feldman.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = MLPClassifier(hidden_layer_sizes=(100, 100), early_stopping=True)\n",
    "# model.fit(train_repd_X, train_repd_y)\n",
    "\n",
    "test_probs = model.predict_proba(test_repd_X)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse unfairness and accuracy\n",
    "\n",
    "We measure the accuracy and fairness in baseline and compare it to the corrected model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bl_test_acc = accuracy(bl_test_probs, test.employed_yes)\n",
    "bl_test_did = disparate_impact_d(bl_test_probs, test.race_white)\n",
    "bl_test_dip = disparate_impact_p(bl_test_probs, test.race_white)\n",
    "\n",
    "test_acc = accuracy(test_probs, test.employed_yes)\n",
    "test_did = disparate_impact_d(test_probs, test.race_white)\n",
    "test_dip = disparate_impact_p(test_probs, test.race_white)\n",
    "\n",
    "print(f\"Baseline accuracy: {bl_test_acc:.3f}\")\n",
    "print(f\"Accuracy: {test_acc:.3f}\\n\")\n",
    "\n",
    "print(f\"Baseline disparate impact (dist.): {bl_test_did:.3f}\")\n",
    "print(f\"Disparate impact (dist.): {test_did:.3f}\\n\")\n",
    "\n",
    "print(f\"Baseline disparate impact (prob.): {bl_test_dip:.3f}\")\n",
    "print(f\"Disparate impact (prob.): {test_dip:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualise the disparity between men and women with a box plot of the scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp_box = group_box_plots(\n",
    "    np.concatenate([bl_test_probs, test_probs]),\n",
    "    np.concatenate([np.zeros_like(bl_test_probs), np.ones_like(test_probs)]),\n",
    "    np.tile(test.race_white.map(lambda x: \"White\" if x else \"Black\"), 2),\n",
    "    group_names=[\"Baseline\", \"Feldman\"],\n",
    ")\n",
    "dp_box"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cdei",
   "language": "python",
   "name": "cdei"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
