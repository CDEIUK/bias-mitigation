{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimal clustering by Zemel et al. - Adult data (race)\n",
    "\n",
    "This notebook contains an implementation of the pre-processing fairness intervention introduced in Learning Fair Representations by Zemel et al. (2013) as part of the IBM AIF360 fairness tool box github.com/IBM/AIF360. \n",
    "\n",
    "Here, we consider fairness defined with respect to race. There is another notebook considering fairness with respect to sex using Zemel et al.'s intervention method, which contains more details on the method. We follow analogous steps to the accompanying notebook addressing unfairness with respect to sex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from aif360.algorithms.preprocessing.lfr import LFR  # noqa\n",
    "from aif360.datasets import StandardDataset\n",
    "from fairlearn.metrics import (\n",
    "    demographic_parity_difference,\n",
    "    demographic_parity_ratio,\n",
    ")\n",
    "from helpers.metrics import accuracy\n",
    "from helpers.plot import group_bar_plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n",
    "We have committed preprocessed data to the repository for reproducibility and we load it here. Check out hte preprocessing notebook for details on how this data was obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artifacts_dir = Path(\"../../../artifacts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = artifacts_dir / \"data\" / \"adult\"\n",
    "\n",
    "train = pd.read_csv(data_dir / \"processed\" / \"train-one-hot.csv\")\n",
    "val = pd.read_csv(data_dir / \"processed\" / \"val-one-hot.csv\")\n",
    "test = pd.read_csv(data_dir / \"processed\" / \"test-one-hot.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove points which are not white/black people\n",
    "\n",
    "In order to analyse unfairness for binary protected attributes, namely, black / white race, we remove data points that correspond to races different to those two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in [train, test, val]:\n",
    "    data = data[data.race_white + data.race_black == 1]\n",
    "    data.drop(\n",
    "        [\n",
    "            \"race_amer_indian_eskimo\",\n",
    "            \"race_asian_pac_islander\",\n",
    "            \"race_other\",\n",
    "            \"race_black\",\n",
    "        ],\n",
    "        axis=1,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to process data for our fairness intervention we need to define special dataset objects which are part of every intervention pipeline within the IBM AIF360 toolbox. These objects contain the original data as well as some useful further information, e.g., which feature is the protected attribute as well as which column corresponds to the label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sds = StandardDataset(\n",
    "    train,\n",
    "    label_name=\"salary\",\n",
    "    favorable_classes=[1],\n",
    "    protected_attribute_names=[\"race_white\"],\n",
    "    privileged_classes=[[1]],\n",
    ")\n",
    "test_sds = StandardDataset(\n",
    "    test,\n",
    "    label_name=\"salary\",\n",
    "    favorable_classes=[1],\n",
    "    protected_attribute_names=[\"race_white\"],\n",
    "    privileged_classes=[[1]],\n",
    ")\n",
    "val_sds = StandardDataset(\n",
    "    val,\n",
    "    label_name=\"salary\",\n",
    "    favorable_classes=[1],\n",
    "    protected_attribute_names=[\"race_white\"],\n",
    "    privileged_classes=[[1]],\n",
    ")\n",
    "index = train_sds.feature_names.index(\"race_white\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "privileged_groups = [{\"race_white\": 1.0}]\n",
    "unprivileged_groups = [{\"race_white\": 0.0}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demographic parity\n",
    "\n",
    "Given the original unfair data set we apply Zemel et al.'s intervention to obtain a fair data set including fair labels. More precisely, we load an already learnt mitigation or learn a new mitigation procedure based on the true and predicted labels of the training data. We then apply the learnt procedure to transform the testing data and analyse fairness and accuracy in the transformed testing data.\n",
    "\n",
    "The degree of fairness and accuracy can be controlled by the choice of parameters $A_x, A_y, A_z$ and $k$ when setting up the mitigation procedure. Here, $A_x$ controls the loss associated with the distance between original and transformed data set, $A_y$ the accuracy loss and $A_z$ the fairness loss. The larger one of these parameter is chosen compared to the others, the larger the priority of minimising the loss associated with that parameter. Hence, leaving $A_x$ and $A_y$ fixed, we can increase the degree of fairness achieved by increasing the parameter $A_z$.\n",
    "\n",
    "As differences in fairness between independently learnt mitigations with same parameter choice can sometimes be significant we load a pre-trained intervention which achieves reasonable results. The user is still encouraged to train inteventions themselves (see commented out code below), and compare achieved fairness, potentially for a number of indepedent runs.\n",
    "\n",
    "## Train unfair model\n",
    "\n",
    "For maximum reproducibility we load the baseline model from disk, but the code used to train can be found in the baseline model notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bl_model = joblib.load(artifacts_dir / \"models\" / \"finance\" / \"baseline.pkl\")\n",
    "\n",
    "bl_test_probs = bl_model.predict_proba(test_sds.features)[:, 1]\n",
    "bl_test_pred = bl_test_probs > 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load or learn intervention\n",
    "\n",
    "So that you can reproduce our results we include a pretrained model, but the code for training your own model and experimenting with hyperparameters can be found below.\n",
    "\n",
    "a) Location of the intervention previously learned on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TR = joblib.load(artifacts_dir / \"models\" / \"finance\" / \"zemel-race.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Learn intervention of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TR = LFR(\n",
    "#     unprivileged_groups=unprivileged_groups,\n",
    "#     privileged_groups=privileged_groups,\n",
    "#     k=5,\n",
    "#     Ax=0.01,\n",
    "#     Ay=1.0,\n",
    "#     Az=100.0,\n",
    "# )\n",
    "# TR = TR.fit(train_sds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply intervention to test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transf_test_sds = TR.transform(test_sds)\n",
    "test_fair_labels = transf_test_sds.labels.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate fairness and accuracy on test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bl_acc = bl_model.score(test.drop(columns=\"salary\"), test.salary)\n",
    "bl_dpd = demographic_parity_difference(\n",
    "    test.salary, bl_test_pred, sensitive_features=test.race_white,\n",
    ")\n",
    "bl_dpr = demographic_parity_ratio(\n",
    "    test.salary, bl_test_pred, sensitive_features=test.race_white,\n",
    ")\n",
    "\n",
    "acc = accuracy(test.salary, test_fair_labels)\n",
    "dpd = demographic_parity_difference(\n",
    "    test.salary, test_fair_labels, sensitive_features=test.race_white,\n",
    ")\n",
    "dpr = demographic_parity_ratio(\n",
    "    test.salary, test_fair_labels, sensitive_features=test.race_white,\n",
    ")\n",
    "\n",
    "print(f\"Baseline accuracy: {bl_acc:.3f}\")\n",
    "print(f\"Accuracy: {acc:.3f}\\n\")\n",
    "\n",
    "print(f\"Baseline demographic parity difference: {bl_dpd:.3f}\")\n",
    "print(f\"Demographic parity difference: {dpd:.3f}\\n\")\n",
    "\n",
    "print(f\"Baseline demographic parity ratio: {bl_dpr:.3f}\")\n",
    "print(f\"Demographic parity ratio: {dpr:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp_bar = group_bar_plots(\n",
    "    np.concatenate([bl_test_pred, test_fair_labels]),\n",
    "    np.tile(test.race_white.map({0: \"Black\", 1: \"White\"}), 2),\n",
    "    groups=np.concatenate(\n",
    "        [np.zeros_like(bl_test_pred), np.ones_like(test_fair_labels)]\n",
    "    ),\n",
    "    group_names=[\"Baseline\", \"Zemel\"],\n",
    "    title=\"Proportion of predicted high earners by race\",\n",
    "    xlabel=\"Propotion of predicted high earners\",\n",
    "    ylabel=\"Method\",\n",
    ")\n",
    "dp_bar"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cdei",
   "language": "python",
   "name": "cdei"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
