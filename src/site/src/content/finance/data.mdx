---
title: The Adult data set
order: 2
---

import { LazyPlot } from "../../components/myplotly"

import salary_by_sex from "../../figures/finance/salary_by_sex.json"
import salary_by_race from "../../figures/finance/salary_by_race.json"
import salary_by_hpw from "../../figures/finance/salary_by_hours_per_week.json"

Before we do any modelling, we should think about what we are trying to achieve and identify possible bias-related risks, as well as whether the data we have available to us is appropriate for the task.

## Purpose identification

Our goal is to try and predict an individual's annual income in order to target marketing for a premium credit card. Since our model is based on demographic information, it is at risk of perpetuating or exacerbating demographic inequality. In particular we should be mindful of possible racial disparities present in the data, which the model could then use for classification. Similarly there is a gender pay gap which is likely to manifest in the data we have available to us, so we are vulnerable to gender bias.

## Data identification

The data we have available to us is the Adult data set from the [UCI Machine learning repository][adult-data]. Contains 14 features:

- `age`: The age of the individual in years,
- `capital_gain`: Capital gain in the previous year,
- `capital_loss`: Capital loss in the previous year,
- `education`: Highest level of education achieved by the individual,
- `education-num`: A numeric form of the highest level of education achieved,
- `fnlwgt`: An estimate of the number of individuals in the population with the same demographics as this individual,
- `hours_per_week`: Hours worked per week,
- `marital_status`: The marital status of the individual,
- `native_country`: The native country of the individual,
- `occupation`: The occupation of the individual,
- `race`: The individual's race,
- `relationship`: The individual's relationship status,
- `sex`: The individual's sex,
- `workclass`: The industry / sector that the individual works in.

In addition to this, the binary label `salary` encodes whether individual earns more or less than \$50,000.

In this case we don't have the means to collect different data. In the real world we would always consider whether we could have access to better data that is more representative. For example, the data we are using was collected in 1994, so it's quite likely that the demographic patterns captured in the data are no longer representative. If we were performing this task for real we would likely recommend that more up to date data is collected, that better reflects demographic shifts in the intervening 26 years.

Another consideration is the representation of different groups in the data. Men outnumber women almost two to one, white respondents outnumber black respondents nine to one, and asian respondents almost thirty to one. This means that our data is not representative of the overall population which isn't ideal, but the bigger problem is that there are relatively few observations of precisely those individuals who are at risk of mistreatment. In order to minimise the risk of bias we should make sure that all groups are well represented in our data.

Since we are merely demonstrating different methods of measuring and mitigating bias we proceed with the data as is, but in a real world application we should see if we can collect more data in order to ensure better representation.

As well as determining whether we could collect more data, we should also consider whether the data we have is appropriate for the task at hand. In this case we decide that the `fnlwgt` feature is not relevant to the task of predicting someone's salary, and drop the `education-num` column as it contains duplicate information also available in the `education` column. Of the remaining features we decide that for now we are happy to include all of them in the model, though we should take care to measure the impact of including features such as `race` or `sex`. As discussed in our report, simply excluding these features is not in any case guaranteed to prevent the resulting model to be unbiased. We scale the continuous ones, and one-hot encode the categorical ones ready for modelling. You can see all of the preprocessing we do [here][binder-data].

Once the data has been preprocessed, we can look at bias that is present in the data, before we have even trained a model. First, we look at the proportion of men and women that earn more than \$50,000 dollars, we see that the proportion is much higher for men.

<LazyPlot {...salary_by_sex} />

Similarly, there is a large disparity in proportion of individuals earning more than \$50,000 across different races.

<LazyPlot {...salary_by_race} />

Finally, we identify a possible "legitimate risk factor" in the `hours_per_week` feature. There is a clear relationship between the number of hours worked per week and the chances that the individual earns more than \$50,000, which stands to reason. Of course we should be careful, because the number of hours worked per week by any one person is itself influenced by societal factors. For example, women are more often than men expected to play the role of a home-maker, and so may not work as many hours.

<LazyPlot {...salary_by_hpw} />

Next we'll try training a model without any attempt to mitigate bias and investigate the results.

[adult-data]: https://archive.ics.uci.edu/ml/datasets/Adult
[binder-data]: https://mybinder.org/v2/gh/imrehg/cdei-development/master?filepath=notebooks%2Ffinance%2Fdata.ipynb "Open notebook on Binder"
