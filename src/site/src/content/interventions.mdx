---
title: Interventions
---

import Collapse from "../components/collapse"
import { LazyPlot } from "../components/siteplotly"

import fin_feld_dp from "../figures/finance/interventions/feldman-dp.json"
import rec_feld_dp from "../figures/recruiting/interventions/feldman-dp.json"

import fin_hardt_eo from "../figures/finance/interventions/hardt-eo.json"
import rec_hardt_eo from "../figures/recruiting/interventions/hardt-eo.json"

import fin_adv_dp from "../figures/finance/interventions/adversarial-dp.json"
import fin_adv_cdp from "../figures/finance/interventions/adversarial-cdp.json"
import fin_adv_blcdp from "../figures/finance/interventions/bl-adversarial-cdp.json"
import fin_adv_eo from "../figures/finance/interventions/adversarial-eo.json"
import fin_adv_bleo from "../figures/finance/interventions/bl-adversarial-eo.json"

Having seen that a model trained without intervention leads to unfair outcomes, we now try to apply some of the mitigation algorithms from the literature. Where possible we use existing open source implementations. Some algorithms we have included our own implementations. All of our analysis can be explored on **BINDER** and the code is all available on GitHub **LINK TO CDEI GITHUB**.

## Feature modification - Feldman et al.

[Feldman et. al][feldman] introduce a pre-processing technique for imposing demographic parity. It is implemented in [IBM's AI Fairness 360][aif-360] library.

<Collapse label="How it works">

The algorithm assumes a binary or categorical protected attribute. It adjusts the distributions of the features so that they are the same in each protected group. For example, in the Adult data set `hours_per_week` is generally lower for women than for men. In this case the algorithm would increase the hours worked per week slightly for women in the dataset, and reduce hours worked per week for men in the data set, in such a way that the two distributions look the same.

The result of applying the algorithm is a modified dataset, such that each feature in the data has been decorrelated from the protected attribute. The idea is that a model trained on this data, should not be able to learn to discriminate based on the protected attributes.

</Collapse>

<Collapse label="Experimental results">

### Finance

We applied Feldman to the adult dataset in order to impose demographic parity with respect to sex. We found that training on the modified data didn't substantially change the results. There was a small drop in accuracy, whereas our baseline achieved 85.3% test set accuracy, the model trained on the fair data achieved 85.0%. However there was also hardly any change in demographic parity difference, going from 0.193 to 0.186. Below we show a box plot of the score distributions for the two models. They appear very similar.

<div
  style={{
    padding: "0 10px",
    margin: "1em 0",
    backgroundColor: "white",
    borderRadius: "8px",
  }}
>
  <LazyPlot {...fin_feld_dp} />
</div>

This is likely because features that are highly correlated with `sex` remain in the data such as `marital-status`.

### Recruiting

We also applied it to the synethetic recruiting data, this time trying to impose demographic parity with respect to race. In this case the intervention was marginally more effective, but still didn't come close to actually achieving fairness. Demographic parity difference decreased from 0.327 in the baseline model to 0.269 for the model trained on fair data. The bar chart of scores shows a modest improvement, but a significant disparity between the races remains.

<div
  style={{
    padding: "0 10px",
    margin: "1em 0",
    backgroundColor: "white",
    borderRadius: "8px",
  }}
>
  <LazyPlot {...rec_feld_dp} />
</div>

</Collapse>

### Summary

In our experiments this intervention was not very effective. Other experiments we have seen with this method achieve better results by doing additional feature selection, possibly there was too much correlation between our features for data modification to work well.

We note that Feldman can be adapted to work as a post-processing technique. This is an unpublished idea that has been [observed by Hardt][hardt-neurips]. The distribution modification algorithm is applied to the scores of an existing model rather than the data. This is a simple but effective strategy for imposing demographic parity.

## Decision threshold modification - Hardt et al.

[Hardt et al.][hardt] introduce a post-processing technique for imposing equalised odds and equal opportunity. It is implemented in [IBM's AI Fairness 360][aif-360] library, and [Microsoft's FairLearn][fairlearn] library.

<Collapse label="How it works">

Equalised odds requires that the true and false positive rates are equal for each protected group. Equal opportunity requires that only the true positive rates are the same. In either case the algorithm achieves this by adjusting the decision thresholds for each group that are used to determine the prediction. In some cases this alone is not enough to achieve equality, in which case two thresholds are set for each group, and the prediction is made by first randomly choosing between the thresholds, then making a prediction with the threshold.

The algorithm is very widely applicable, as it only needs access to the model outputs and the protected attribute. Moreover Hardt et al. show that their algorithm is optimal among post-processing algorithms for equalised odds. However, the possible randomness present in predictions may not be satisfactory when individual fairness is a concern, as two identical individuals could receive different predictions due to the stochasticity.

</Collapse>

<Collapse label="Experimental results">

### Finance

We applied Hardt to the adult dataset in order to impose equalised odds with respect to sex. The intervention is extremely effective, the test set equalised odds difference is negligible, while the test set accuracy fell about three percentage points compared to the baseline.

<div
  style={{
    padding: "0 10px",
    margin: "1em 0",
    backgroundColor: "white",
    borderRadius: "8px",
  }}
>
  <LazyPlot {...fin_hardt_eo} />
</div>

### Recruiting

We also applied it to the synethetic recruiting data, this time trying to impose equalised odds with respect to race. Again it was extremely effective, with test set equalised odds difference being negligible, and test set accuracy falling about three percentage points.

<div
  style={{
    padding: "0 10px",
    margin: "1em 0",
    backgroundColor: "white",
    borderRadius: "8px",
  }}
>
  <LazyPlot {...rec_hardt_eo} />
</div>

</Collapse>

### Summary

The algorithm of Hardt et al. is extremely effective, which is not surprising as they prove in their paper that their intervention is optimal among post-processing algorithms for equalised odds.

There are perhaps two drawbacks. The first is that it achieves fairness through some randomisation of decision thresholds, which means that the post-processed classifier can fail individual fairness badly. In fact two identical individuals could receive different outcomes. The second is that it fully mitigates bias, which can have a negative performance implications. It is not possible to balance fairness and accuracy requirements by reducing the bias partially but not fully.

## Kamiran

Here we give details on the fairness intervention by Kamiran et al. (2012) which relabels data points for which the model prediction is not decisive. Such data points are assigned the positive label if they belong to the unprivileged group and the negative label otherwise.

## Kamiran - Calders

## Kamishima

## Pleiss

## Zemel

## Zhang

The paper [Mitigating Unwanted Biases with Adversarial Learning][zhang] of Zhang et al. introduces a method for mitigating bias in a model using adversarial learning. Their approach is able to impose demographic parity, conditional demographic parity, and equalised odds with only minor modifications. There is an implementation in [IBM's AI Fairness 360][aif-360] library, but it can only address demographic parity. Hence we provide our own implementation for comparing its performance across different definitions of fairness.

<Collapse label="How it works">

The model is trained in tandem with an adversary, which we refer to as the discriminator. The discriminator monitors the model output, and tries to predict the protected attributes. If it were able to do so, this would be a sign that the model is treating the protected groups differently. Hence the model is trained to simultaneously optimise a performance objective and to fool the discriminator. If it learns to fool the discriminator, then the model outputs are unbiased.

To achieve conditional demographic parity we additionally pass legitimate risk factors to the discriminator, so that the model receives no benefit from removing information about the protected attributes from its output that is contained in those factors. Similarly to achieve equalised odds we allow the discriminator to additionally see the labels during training, so that the model is not incentivised to remove from its output any information about the sensitive data that is contained in the labels.

</Collapse>

### Demographic parity

This algorithm is very effective at imposing demographic parity. With minimal tuning we saw a significant increase in demographic parity and a minimal decrease in accuracy.

<LazyPlot {...adv_dp} />

### Conditional demographic parity

Similarly imposing conditional demographic parity with respect to `hours_per_week` with this approach proved very effective. Again without much tuning we saw a significant improvement.

<LazyPlot {...adv_blcdp} />
<LazyPlot {...adv_cdp} />

### Equal opportunity

Equal opportunity proved more challenging. We saw some improvement, but if anything the model somewhat over-corrected. The core problem appears to be that the tension between performance and fairness constraints leads to some instability during training that is typical of adversarial methods. Zhang et al. recommend a few possible strategies for addressing these problems, such as modifying the discriminator loss weight over time so as to slowly increase the penalty for unfairness. In our implementation we warm up without a fairness constraint, but then turn on the fairness constraint suddenly rather than slowly increase it, so there are things we could have done differently to address some of the training issues, however it's clear that imposing equalised odds with this algorithm is more delicate than other definitions of fairness.

<LazyPlot {...adv_bleo} />
<LazyPlot {...adv_eo} />

## Conclusions

Say something really smart here.

[aif-360]: https://aif360.readthedocs.io/en/latest/
[fairlearn]: https://fairlearn.github.io/
[feldman]: https://dl.acm.org/doi/10.1145/2783258.2783311
[hardt]: https://papers.nips.cc/paper/6374-equality-of-opportunity-in-supervised-learning.pdf
[hardt-neurips]: https://mrtz.org/nips17/#/41
[zhang]: https://dl.acm.org/doi/10.1145/3278721.3278779
