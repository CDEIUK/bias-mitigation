---
title: Interventions
---

import Collapse from "../components/collapse"
import { LazyPlot } from "../components/siteplotly"

import adv_dp from "../figures/finance/interventions/adversarial-dp.json"
import adv_cdp from "../figures/finance/interventions/adversarial-cdp.json"
import adv_blcdp from "../figures/finance/interventions/bl-adversarial-cdp.json"
import adv_eo from "../figures/finance/interventions/adversarial-eo.json"
import adv_bleo from "../figures/finance/interventions/bl-adversarial-eo.json"

Having seen that a model trained without intervention leads to unfair outcomes, we now try to apply some of the mitigation algorithms from the literature. Where possible we use existing open source implementations. Some algorithms we have included our own implementations. All of our analysis can be explored on **BINDER** and the code is all available on GitHub **LINK TO CDEI GITHUB**.

## Feldman

Feldman et. al introduce a pre-processing technique for imposing demographic parity. It is implemented in [IBM's AI Fairness 360][aif-360] library.

<Collapse label="How it works">

The algorithm assumes a binary or categorical protected attribute. It adjusts the distributions of the features so that they are similar for each protected group. For example, if `hours_per_week` is different for men and women, the disparate impact remover would adjust the data to bring the two distributions in line. This can be done for example by inverting the cumulative distribution function.

</Collapse>

## Hardt

Hardt et al introduce a post-processing technique for imposing equalised odds and equal opportunity. It is implemented in [IBM's AI Fairness 360][aif-360] library, and [Microsoft's FairLearn][fairlearn] library.

<Collapse label="How it works">

Equalised odds requires that the true and false positive rates are equal for each protected group. Equal opportunity requires that only the true positive rates are the same. In either case the algorithm achieves this by adjusting the decision thresholds for each group that are used to determine the prediction. In some cases this alone is not enough to achieve equality, in which case two thresholds are set for each group, and the prediction is made by first randomly choosing between the thresholds, then making a prediction with the threshold.

The algorithm is very widely applicable, as it only needs access to the model outputs and the protected attribute. Moreover Hardt et al. show that their algorithm is optimal among post-processing algorithms for equalised odds. However, the possible randomness present in predictions may not be satisfactory when individual fairness is a concern, as two identical individuals could receive different predictions due to the stochasticity.

</Collapse>

## Kamiran

Here we give details on the fairness intervention by Kamiran et al. (2012) which relabels data points for which the model prediction is not decisive. Such data points are assigned the positive label if they belong to the unprivileged group and the negative label otherwise.

## Kamiran - Calders

## Kamishima

## Pleiss

## Zemel

## Zhang

The paper [Mitigating Unwanted Biases with Adversarial Learning][zhang] of Zhang et al. introduces a method for mitigating bias in a model using adversarial learning. Their approach is able to impose demographic parity, conditional demographic parity, and equalised odds with only minor modifications. There is an implementation in [IBM's AI Fairness 360][aif-360] library, but it can only address demographic parity. Hence we provide our own implementation for comparing its performance across different definitions of fairness.

<Collapse label="How it works">

The model is trained in tandem with an adversary, which we refer to as the discriminator. The discriminator monitors the model output, and tries to predict the protected attributes. If it were able to do so, this would be a sign that the model is treating the protected groups differently. Hence the model is trained to simultaneously optimise a performance objective and to fool the discriminator. If it learns to fool the discriminator, then the model outputs are unbiased.

To achieve conditional demographic parity we additionally pass legitimate risk factors to the discriminator, so that the model receives no benefit from removing information about the protected attributes from its output that is contained in those factors. Similarly to achieve equalised odds we allow the discriminator to additionally see the labels during training, so that the model is not incentivised to remove from its output any information about the sensitive data that is contained in the labels.

</Collapse>

### Demographic parity

This algorithm is very effective at imposing demographic parity. With minimal tuning we saw a significant increase in demographic parity and a minimal decrease in accuracy.

<LazyPlot {...adv_dp} />

### Conditional demographic parity

Similarly imposing conditional demographic parity with respect to `hours_per_week` with this approach proved very effective. Again without much tuning we saw a significant improvement.

<LazyPlot {...adv_blcdp} />
<LazyPlot {...adv_cdp} />

### Equal opportunity

Equal opportunity proved more challenging. We saw some improvement, but if anything the model somewhat over-corrected. The core problem appears to be that the tension between performance and fairness constraints leads to some instability during training that is typical of adversarial methods. Zhang et al. recommend a few possible strategies for addressing these problems, such as modifying the discriminator loss weight over time so as to slowly increase the penalty for unfairness. In our implementation we warm up without a fairness constraint, but then turn on the fairness constraint suddenly rather than slowly increase it, so there are things we could have done differently to address some of the training issues, however it's clear that imposing equalised odds with this algorithm is more delicate than other definitions of fairness.

<LazyPlot {...adv_bleo} />
<LazyPlot {...adv_eo} />

## Conclusions

Say something really smart here.

[aif-360]: https://aif360.readthedocs.io/en/latest/
[fairlearn]: https://fairlearn.github.io/
[zhang]: https://dl.acm.org/doi/10.1145/3278721.3278779
