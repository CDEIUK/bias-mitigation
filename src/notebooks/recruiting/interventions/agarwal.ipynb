{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reductions approach by Agarwal et al. - Recruiting data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the implementation of the in-processing algorithm introduced in [A Reductions Approach to Fair Classification\n",
    "](http://proceedings.mlr.press/v80/agarwal18a.html) by Pleiss et al. (2018) as part of the [FairLearn tool box](https://github.com/fairlearn/).\n",
    "\n",
    "Ihe intervention achieves a fair classification as the minimisation of the prediction error under a general form of linear constraint, which addresses Demographic Parity and Equalised Odds as special cases. The optimisation is solved by a sequence of cost-sensitive classification problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from fairlearn.metrics import (\n",
    "    equalized_odds_difference,\n",
    "    demographic_parity_difference,\n",
    ")\n",
    "from fairlearn.reductions import ExponentiatedGradient  # noqa\n",
    "from fairlearn.reductions import (\n",
    "    DemographicParity,\n",
    "    EqualizedOdds,\n",
    "    TruePositiveRateDifference,\n",
    ")\n",
    "from helpers.metrics import accuracy\n",
    "from helpers.plot import group_bar_plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "export"
    ]
   },
   "outputs": [],
   "source": [
    "from helpers import export_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "We have committed preprocessed data to the repository for reproducibility and we load it here. Check out hte preprocessing notebook for details on how this data was obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artifacts_dir = Path(\"../../../artifacts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "export"
    ]
   },
   "outputs": [],
   "source": [
    "# override data_dir in source notebook\n",
    "# this is stripped out for the hosted notebooks\n",
    "artifacts_dir = Path(\"../../../../artifacts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = artifacts_dir / \"data\" / \"recruiting\"\n",
    "\n",
    "train = pd.read_csv(data_dir / \"processed\" / \"train.csv\")\n",
    "val = pd.read_csv(data_dir / \"processed\" / \"val.csv\")\n",
    "test = pd.read_csv(data_dir / \"processed\" / \"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "race = train.drop(\"employed_yes\", axis=1)[\"race_white\"].apply(\n",
    "    lambda race_white: \"black\" if race_white == 0 else \"white\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load original model\n",
    "\n",
    "For maximum reproducibility we can also load the baseline model from disk, but the code used to train can be found in the baseline model notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model = joblib.load(\n",
    "    artifacts_dir / \"models\" / \"recruiting\" / \"baseline.pkl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get predictions on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bl_test_probs = baseline_model.predict_proba(\n",
    "    test.drop(\"employed_yes\", axis=1)\n",
    ")[:, 1]\n",
    "bl_test_labels = (bl_test_probs > 0.5).astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demographic parity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first address demographic parity. In order to do so, we learn the reductions algorithm based on the true labels of the training data. We then apply the learnt fair calssifier to predict the test data labels and analyse the outcomes for fairness and accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constraint = DemographicParity()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learn intervention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the FairLearn implementation of Agarwal et al. requires the classifier it will be learning to have a sample_weight argument, we cannot learn the type of neural net our baseline model is based on with this approach. Instead we choose a random forest classifier, and learn it on the training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = RandomForestClassifier(n_estimators=500, max_depth=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the training procedure is lengthy we load the resulting predicted labels on the test data from a previously learnt fair model. The user is encouraged to reproduce these results however by running the commented out code for training the fair model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_labels = np.load(\n",
    "    artifacts_dir / \"models\" / \"recruiting\" / \"agarwal_dp.npy\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For maximum reproducability we set the random seed. This makes sure we generate the same model from the training procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The underlying implementation applies the exponential gradient reduction algorithm for fair classification from Agarwal et al."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mitigator = ExponentiatedGradient(classifier, constraint)\n",
    "# mitigator.fit(\n",
    "#     train.drop(\"employed_yes\", axis=1),\n",
    "#     train.employed_yes.values,\n",
    "#     sensitive_features=race,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate predictions from fair classifier on test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_pred_labels = mitigator.predict(test.drop(\"employed_yes\", axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyse fairness and accuracy on test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_race = test.race_white.values\n",
    "test_employed = test.employed_yes.values\n",
    "mask = test_race == 1\n",
    "\n",
    "# baseline metrics\n",
    "bl_test_acc = accuracy(test_employed, bl_test_probs)\n",
    "bl_test_dpd = demographic_parity_difference(\n",
    "    test_employed, bl_test_labels, sensitive_features=test_race,\n",
    ")\n",
    "\n",
    "# new model metrics\n",
    "test_acc = accuracy(test_employed, test_pred_labels)\n",
    "test_dpd = demographic_parity_difference(\n",
    "    test_employed, test_pred_labels, sensitive_features=test_race,\n",
    ")\n",
    "\n",
    "print(f\"Baseline accuracy: {bl_test_acc:.3f}\")\n",
    "print(f\"Accuracy: {test_acc:.3f}\\n\")\n",
    "\n",
    "print(f\"Baseline demographic parity: {bl_test_dpd:.3f}\")\n",
    "print(f\"Demographic parity: {test_dpd:.3f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider accuracy on the female / male subgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "white_acc = accuracy(\n",
    "    test_pred_labels[test.race_white == 1],\n",
    "    test.employed_yes[test.race_white == 1],\n",
    ")\n",
    "black_acc = accuracy(\n",
    "    test_pred_labels[test.race_white == 0],\n",
    "    test.employed_yes[test.race_white == 0],\n",
    ")\n",
    "\n",
    "mean_black_score = test_pred_labels[test.race_white == 0].mean()\n",
    "mean_white_score = test_pred_labels[test.race_white == 1].mean()\n",
    "\n",
    "print(f\"Black accuracy: {black_acc:.3f}\")\n",
    "print(f\"White accuracy: {white_acc:.3f}\")\n",
    "print(f\"Mean black score: {mean_black_score:.3f}\")\n",
    "print(f\"Mean white score: {mean_white_score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp_bar = group_bar_plots(\n",
    "    test_pred_labels,\n",
    "    test.race_white.map({0: \"Black\", 1: \"White\"}),\n",
    "    title=\"Predictions by race\",\n",
    "    xlabel=\"Proportion predicted successful\",\n",
    ")\n",
    "dp_bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "export"
    ]
   },
   "outputs": [],
   "source": [
    "export_plot(dp_bar, \"agarwal-dp.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Equalised odds\n",
    "\n",
    "We'll now repeat the process for equalised odds, which requires us changing the constraint and leads to learning a fair classifier addressing equalised odds. There are no further modifcations to the existing parameter choices required. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constraint = EqualizedOdds()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we load the the resulting test label predictions from a previously learnt model. The code that generated that model is commented out below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_labels = np.load(\n",
    "    artifacts_dir / \"models\" / \"recruiting\" / \"agarwal_eo.npy\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learn intervention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.seed(42)\n",
    "# mitigator = ExponentiatedGradient(classifier, constraint)\n",
    "# mitigator.fit(\n",
    "#     train.drop(\"employed_yes\", axis=1),\n",
    "#     train.employed_yes.values,\n",
    "#     sensitive_features=race,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate predictions from fair classifier test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_pred_labels = mitigator.predict(test.drop(\"employed_yes\", axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyse fairness and accuracy on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_race = test.race_white.values\n",
    "test_employed = test.employed_yes.values\n",
    "mask = test_race == 1\n",
    "\n",
    "# baseline metrics\n",
    "bl_test_acc = accuracy(test_employed, bl_test_probs)\n",
    "bl_test_eod = equalized_odds_difference(\n",
    "    test_employed, bl_test_labels, sensitive_features=test_race,\n",
    ")\n",
    "\n",
    "# new model metrics\n",
    "test_acc = accuracy(test_employed, test_pred_labels)\n",
    "test_eod = equalized_odds_difference(\n",
    "    test_employed, test_pred_labels, sensitive_features=test_race,\n",
    ")\n",
    "\n",
    "print(f\"Baseline accuracy: {bl_test_acc:.3f}\")\n",
    "print(f\"Accuracy: {test_acc:.3f}\\n\")\n",
    "\n",
    "print(f\"Baseline equalised odds (dist.): {bl_test_eod:.3f}\")\n",
    "print(f\"Equalised odds (dist.): {test_eod:.3f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eo_bar = group_bar_plots(\n",
    "    test_pred_labels,\n",
    "    test.race_white.map({0: \"Black\", 1: \"White\"}),\n",
    "    groups=test.employed_yes,\n",
    "    group_names=[\"Not employed\", \"Employed\"],\n",
    "    title=\"Predictions by race and outcome\",\n",
    "    xlabel=\"Proportion predicted successful\",\n",
    "    ylabel=\"Outcome\",\n",
    ")\n",
    "eo_bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "export"
    ]
   },
   "outputs": [],
   "source": [
    "export_plot(eo_bar, \"agarwal-eo.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Equal opportunity\n",
    "We'll now repeat the process for equalised opportunity, which only requires us changing the constraint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constraint = TruePositiveRateDifference()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we load the the resulting test label predictions from a previously learnt model. The code that generated that model is commented out below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_labels = np.load(\n",
    "    artifacts_dir / \"models\" / \"recruiting\" / \"agarwal_eopp.npy\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learn intervention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.seed(42)\n",
    "# mitigator = ExponentiatedGradient(classifier, constraint)\n",
    "# mitigator.fit(\n",
    "#     train.drop(\"employed_yes\", axis=1),\n",
    "#     train.employed_yes.values,\n",
    "#     sensitive_features=race,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate predictions from fair classifier test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_pred_labels = mitigator.predict(test.drop(\"employed_yes\", axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyse fairness and accuracy on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_race = test.race_white.values\n",
    "test_employed = test.employed_yes.values\n",
    "mask = test_race == 1\n",
    "\n",
    "# baseline metrics\n",
    "bl_test_acc = accuracy(test_employed, bl_test_probs)\n",
    "bl_test_eoppd = equalized_odds_difference(\n",
    "    test_employed[test.employed_yes == 1],\n",
    "    bl_test_labels[test.employed_yes == 1],\n",
    "    sensitive_features=test_race[test.employed_yes == 1],\n",
    ")\n",
    "\n",
    "# new model metrics\n",
    "test_acc = accuracy(test_employed, test_pred_labels)\n",
    "test_eoppd = equalized_odds_difference(\n",
    "    test_employed[test.employed_yes == 1],\n",
    "    test_pred_labels[test.employed_yes == 1],\n",
    "    sensitive_features=test_race[test.employed_yes == 1],\n",
    ")\n",
    "\n",
    "print(f\"Baseline accuracy: {bl_test_acc:.3f}\")\n",
    "print(f\"Accuracy: {test_acc:.3f}\\n\")\n",
    "\n",
    "print(f\"Baseline equal opportunity: {bl_test_eoppd:.3f}\")\n",
    "print(f\"Equal opportunity: {test_eoppd:.3f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eopp_bar = group_bar_plots(\n",
    "    test_pred_labels[test.employed_yes == 1],\n",
    "    test.race_white[test.employed_yes == 1].map({0: \"Black\", 1: \"White\"}),\n",
    "    title=\"Predictions by race for successful applicants\",\n",
    "    xlabel=\"Proportion predicted successful\",\n",
    ")\n",
    "eopp_bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "export"
    ]
   },
   "outputs": [],
   "source": [
    "export_plot(eopp_bar, \"agarwal-eopp.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python [conda env:Python3]",
   "language": "python",
   "name": "conda-env-Python3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
