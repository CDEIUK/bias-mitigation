{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kamiran"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objs as go\n",
    "from aif360.datasets import StandardDataset\n",
    "from aif360.algorithms.postprocessing.reject_option_classification import (\n",
    "    RejectOptionClassification,\n",
    ")\n",
    "from helpers import export_plot\n",
    "from helpers.fairness_measures import *\n",
    "from helpers.finance import preprocess\n",
    "from helpers.plot import group_box_plots, group_roc_curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artifacts_dir = Path(\"../../../artifacts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "export"
    ]
   },
   "outputs": [],
   "source": [
    "# override data_dir in source notebook\n",
    "# this is stripped out for the hosted notebooks\n",
    "artifacts_dir = Path(\"../../../../artifacts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = artifacts_dir / \"data\" / \"adult\"\n",
    "preprocess(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(data_dir / \"processed\" / \"train-one-hot.csv\").sample(6000)\n",
    "val = pd.read_csv(data_dir / \"processed\" / \"val-one-hot.csv\").sample(2000)\n",
    "test = pd.read_csv(data_dir / \"processed\" / \"test-one-hot.csv\").sample(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sds = StandardDataset(\n",
    "    train,\n",
    "    label_name=\"salary\",\n",
    "    favorable_classes=[1],\n",
    "    protected_attribute_names=[\"sex\"],\n",
    "    privileged_classes=[[1]],\n",
    ")\n",
    "test_sds = StandardDataset(\n",
    "    test,\n",
    "    label_name=\"salary\",\n",
    "    favorable_classes=[1],\n",
    "    protected_attribute_names=[\"sex\"],\n",
    "    privileged_classes=[[1]],\n",
    ")\n",
    "val_sds = StandardDataset(\n",
    "    val,\n",
    "    label_name=\"salary\",\n",
    "    favorable_classes=[1],\n",
    "    protected_attribute_names=[\"sex\"],\n",
    "    privileged_classes=[[1]],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "privileged_groups = [{\"sex\": 1.0}]\n",
    "unprivileged_groups = [{\"sex\": 0.0}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train unfair model\n",
    "\n",
    "For maximum reproducibility we load the baseline model from disk, but the code used to train can be found in the baseline model notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bl_model = joblib.load(artifacts_dir / \"models\" / \"finance\" / \"baseline.pkl\")\n",
    "\n",
    "bl_test_probs = bl_model.predict_proba(test_sds.features)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_scores = bl_model.predict_proba(val.drop(\"salary\", axis=1))[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Original model accuracy =\", accuracy(val_scores, val.salary))\n",
    "print(\n",
    "    \"Female accuracy =\",\n",
    "    accuracy(val_scores[val.sex == 0], val.salary[val.sex == 0]),\n",
    ")\n",
    "print(\n",
    "    \"Male accuracy =\",\n",
    "    accuracy(val_scores[val.sex == 1], val.salary[val.sex == 1]),\n",
    ")\n",
    "print(\"Mean female score =\", val_scores[val.sex == 0].mean())\n",
    "print(\"Mean male score =\", val_scores[val.sex == 1].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_sds_pred = val_sds.copy(deepcopy=True)\n",
    "val_sds_pred.scores = val_scores.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform intervention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find best threshold for classification only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aif360.metrics import ClassificationMetric\n",
    "\n",
    "num_thresh = 100\n",
    "ba_arr = np.zeros(num_thresh)\n",
    "class_thresh_arr = np.linspace(0.01, 0.99, num_thresh)\n",
    "for idx, class_thresh in enumerate(class_thresh_arr):\n",
    "\n",
    "    fav_inds = val_sds_pred.scores > class_thresh\n",
    "    val_sds_pred.labels[fav_inds] = val_sds_pred.favorable_label\n",
    "    val_sds_pred.labels[~fav_inds] = val_sds_pred.unfavorable_label\n",
    "\n",
    "    classified_metric_orig_valid = ClassificationMetric(\n",
    "        val_sds,\n",
    "        val_sds_pred,\n",
    "        unprivileged_groups=unprivileged_groups,\n",
    "        privileged_groups=privileged_groups,\n",
    "    )\n",
    "\n",
    "    ba_arr[idx] = 0.5 * (\n",
    "        classified_metric_orig_valid.true_positive_rate()\n",
    "        + classified_metric_orig_valid.true_negative_rate()\n",
    "    )\n",
    "\n",
    "best_ind = np.where(ba_arr == np.max(ba_arr))[0][0]\n",
    "best_class_thresh = class_thresh_arr[best_ind]\n",
    "\n",
    "print(\n",
    "    \"Best balanced accuracy (no fairness constraints) = %.4f\" % np.max(ba_arr)\n",
    ")\n",
    "print(\n",
    "    \"Optimal classification threshold (no fairness constraints) = %.4f\"\n",
    "    % best_class_thresh\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metric used (should be one of allowed_metrics)\n",
    "metric_name = \"Statistical parity difference\"\n",
    "\n",
    "# Upper and lower bound on the fairness metric used\n",
    "metric_ub = 0.05\n",
    "metric_lb = -0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimate optimal parameters in ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROC = RejectOptionClassification(\n",
    "    unprivileged_groups=unprivileged_groups,\n",
    "    privileged_groups=privileged_groups,\n",
    "    low_class_thresh=0.01,\n",
    "    high_class_thresh=0.99,\n",
    "    num_class_thresh=100,\n",
    "    num_ROC_margin=50,\n",
    "    metric_name=metric_name,\n",
    "    metric_ub=metric_ub,\n",
    "    metric_lb=metric_lb,\n",
    ")\n",
    "ROC = ROC.fit(val_sds, val_sds_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"Optimal classification threshold (with fairness constraints) = %.4f\"\n",
    "    % ROC.classification_threshold\n",
    ")\n",
    "print(\"Optimal ROC margin = %.4f\" % ROC.ROC_margin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions from validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics for the test set\n",
    "fav_inds = val_sds_pred.scores > best_class_thresh\n",
    "val_sds_pred.labels[fav_inds] = val_sds_pred.favorable_label\n",
    "val_sds_pred.labels[~fav_inds] = val_sds_pred.unfavorable_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply intervention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the validation set\n",
    "val_sds_pred_transf = ROC.predict(val_sds_pred).copy(deepcopy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse fairness and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy =\", accuracy(val_sds_pred_transf.labels.flatten(), val.salary))\n",
    "print(\n",
    "    \"Female accuracy =\",\n",
    "    accuracy(\n",
    "        val_sds_pred_transf.labels.flatten()[val.sex == 0],\n",
    "        val.salary[val.sex == 0],\n",
    "    ),\n",
    ")\n",
    "print(\n",
    "    \"Male accuracy =\",\n",
    "    accuracy(\n",
    "        val_sds_pred_transf.labels.flatten()[val.sex == 1],\n",
    "        val.salary[val.sex == 1],\n",
    "    ),\n",
    ")\n",
    "print(\n",
    "    \"Mean female score =\",\n",
    "    val_sds_pred_transf.labels.flatten()[val.sex == 0].mean(),\n",
    ")\n",
    "print(\n",
    "    \"Mean male score =\",\n",
    "    val_sds_pred_transf.labels.flatten()[val.sex == 1].mean(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp_bar = go.Figure(\n",
    "    data=[\n",
    "        go.Bar(\n",
    "            x=[sex],\n",
    "            y=[val_sds_pred_transf.labels.flatten()[val.sex == sex].mean()],\n",
    "            name=\"Male\" if sex else \"Female\",\n",
    "        )\n",
    "        for sex in range(2)\n",
    "    ],\n",
    "    layout={\"yaxis\": {\"range\": [0, 1]}}\n",
    ")\n",
    "dp_bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "export"
    ]
   },
   "outputs": [],
   "source": [
    "export_plot(dp_bar, \"kamiran-dp.json\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "cdei",
   "language": "python",
   "name": "cdei"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
