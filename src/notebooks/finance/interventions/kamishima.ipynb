{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularisation by Kamishima - Adult data\n",
    "\n",
    "This notebook contains the implementation of the in-processing fairness intervention introduced in [Fairness-Aware Classifier with Prejudice Remover Regularizer](https://link.springer.com/chapter/10.1007/978-3-642-33486-3_3) by Kamishima et al. (2012) as part of the IBM AIF360 fairness tool box github.com/IBM/AIF360.\n",
    "\n",
    "The intervention achieves demographic parity in a logistic regression classifier which is based on maximising the sum between utility expressed via probabilities of classifying data points correctly given their features and further a regularisation term that incorporates the level of unfairness in the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from aif360.datasets import StandardDataset\n",
    "from aif360.algorithms.inprocessing import PrejudiceRemover  # noqa\n",
    "from fairlearn.metrics import demographic_parity_difference\n",
    "from helpers.metrics import accuracy\n",
    "from helpers.plot import group_box_plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "export"
    ]
   },
   "outputs": [],
   "source": [
    "from helpers import export_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n",
    "We have committed preprocessed data to the repository for reproducibility and we load it here. Check out hte preprocessing notebook for details on how this data was obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artifacts_dir = Path(\"../../../artifacts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "export"
    ]
   },
   "outputs": [],
   "source": [
    "# override data_dir in source notebook\n",
    "# this is stripped out for the hosted notebooks\n",
    "artifacts_dir = Path(\"../../../../artifacts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = artifacts_dir / \"data\" / \"adult\"\n",
    "\n",
    "train = pd.read_csv(data_dir / \"processed\" / \"train-one-hot.csv\")\n",
    "val = pd.read_csv(data_dir / \"processed\" / \"val-one-hot.csv\")\n",
    "test = pd.read_csv(data_dir / \"processed\" / \"test-one-hot.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to process data for our fairness intervention we need to define special dataset objects which are part of every intervention pipeline within the IBM AIF360 toolbox. These objects contain the original data as well as some useful further information, e.g., which feature is the protected attribute as well as which column corresponds to the label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sds = StandardDataset(\n",
    "    train,\n",
    "    label_name=\"salary\",\n",
    "    favorable_classes=[1],\n",
    "    protected_attribute_names=[\"sex\"],\n",
    "    privileged_classes=[[1]],\n",
    ")\n",
    "test_sds = StandardDataset(\n",
    "    test,\n",
    "    label_name=\"salary\",\n",
    "    favorable_classes=[1],\n",
    "    protected_attribute_names=[\"sex\"],\n",
    "    privileged_classes=[[1]],\n",
    ")\n",
    "val_sds = StandardDataset(\n",
    "    val,\n",
    "    label_name=\"salary\",\n",
    "    favorable_classes=[1],\n",
    "    protected_attribute_names=[\"sex\"],\n",
    "    privileged_classes=[[1]],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train unfair model\n",
    "\n",
    "For maximum reproducibility we load the baseline model from disk, but the code used to train can be found in the baseline model notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bl_model = joblib.load(artifacts_dir / \"models\" / \"finance\" / \"baseline.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get predictions for the validation and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bl_test_probs = bl_model.predict_proba(test.drop(\"salary\", axis=1))[:, 1]\n",
    "bl_test_pred = bl_test_probs > 0.5\n",
    "test_sds_pred = test_sds.copy(deepcopy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demographic parity\n",
    "\n",
    "Here, we address demographic parity by learning a fair logisitc regression on the trainig data. Subsequently, we apply the learnt model to the test data and analyse the outcomes for fairness and accuracy. The model training allows the specification of a parameter $\\eta$, which controls the loss in accuracy for an increase in fairness. The larger eta the higher the obtained fairness on average. \n",
    "\n",
    "The user is encouraged to consider the influence of the choice of the parameter $\\eta$. However, since the learning of the fair model is for some choice of $\\eta$ quite slow, we generated the following predictions with $\\eta=10$. Since we cannot save the model we saved the predictions generated by the model instead. Since the training is deterministic, the same choice of $\\eta$ should lead to same results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PR = PrejudiceRemover(eta=10.0, sensitive_attr=\"sex\", class_attr=\"salary\")\n",
    "# PR.fit(train_sds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply model on test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_sds_pred = PR.predict(test_sds)\n",
    "# test_probs = test_sds_pred.scores.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_probs = np.load(\n",
    "    artifacts_dir / \"models\" / \"finance\" / \"kamishima_test_scores.npy\"\n",
    ")\n",
    "test_pred = test_probs > 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyse fairness and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features = test.drop(columns=[\"sex\", \"salary\"]).values\n",
    "test_sex = test.sex.values\n",
    "test_salary = test.salary.values\n",
    "mask = test_sex == 1\n",
    "\n",
    "# baseline metrics\n",
    "bl_test_acc = accuracy(test_salary, bl_test_probs)\n",
    "bl_test_dpd = demographic_parity_difference(\n",
    "    test.salary, bl_test_pred, sensitive_features=test_sex,\n",
    ")\n",
    "\n",
    "# new model metrics\n",
    "test_acc = accuracy(test_salary, test_pred)\n",
    "test_dpd = demographic_parity_difference(\n",
    "    test.salary, test_pred, sensitive_features=test_sex,\n",
    ")\n",
    "\n",
    "print(f\"Baseline accuracy: {bl_test_acc:.3f}\")\n",
    "print(f\"Accuracy: {test_acc:.3f}\\n\")\n",
    "\n",
    "print(f\"Baseline demographic parity: {bl_test_dpd:.3f}\")\n",
    "print(f\"Demographic parity: {test_dpd:.3f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp_box = group_box_plots(\n",
    "    np.concatenate([bl_test_probs, test_probs]),\n",
    "    np.tile(test.sex.map({0: \"Female\", 1: \"Male\"}), 2),\n",
    "    groups=np.concatenate(\n",
    "        [np.zeros_like(bl_test_probs), np.ones_like(test_probs)]\n",
    "    ),\n",
    "    group_names=[\"Baseline\", \"Kamishima\"],\n",
    "    title=\"Distribution of scores by sex\",\n",
    "    xlabel=\"Scores\",\n",
    "    ylabel=\"Method\",\n",
    ")\n",
    "dp_box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "export"
    ]
   },
   "outputs": [],
   "source": [
    "export_plot(dp_box, \"kamishima-dp.json\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "cdei",
   "language": "python",
   "name": "cdei"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
