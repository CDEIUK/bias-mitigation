{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.externals import joblib\n",
    "from cdei_helpers.fairness_measures import *\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objs as go\n",
    "from cdei_helpers.plot import group_box_plots, group_roc_curves\n",
    "\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    x,\n",
    "    y,\n",
    "    x_control,\n",
    "    loss_function,\n",
    "    apply_fairness_constraints,\n",
    "    apply_accuracy_constraint,\n",
    "    sep_constraint,\n",
    "    sensitive_attrs,\n",
    "    sensitive_attrs_to_cov_thresh,\n",
    "    gamma=None,\n",
    "):\n",
    "\n",
    "    \"\"\"\n",
    "    Function that trains the model subject to various fairness constraints.\n",
    "    If no constraints are given, then simply trains an unaltered classifier.\n",
    "    Example usage in: \"synthetic_data_demo/decision_boundary_demo.py\"\n",
    "    ----\n",
    "    Inputs:\n",
    "    X: (n) x (d+1) numpy array -- n = number of examples, d = number of features, one feature is the intercept\n",
    "    y: 1-d numpy array (n entries)\n",
    "    x_control: dictionary of the type {\"s\": [...]}, key \"s\" is the sensitive feature name, and the value is a 1-d list with n elements holding the sensitive feature values\n",
    "    loss_function: the loss function that we want to optimize -- for now we have implementation of logistic loss, but other functions like hinge loss can also be added\n",
    "    apply_fairness_constraints: optimize accuracy subject to fairness constraint (0/1 values)\n",
    "    apply_accuracy_constraint: optimize fairness subject to accuracy constraint (0/1 values)\n",
    "    sep_constraint: apply the fine grained accuracy constraint\n",
    "        for details, see Section 3.3 of arxiv.org/abs/1507.05259v3\n",
    "        For examples on how to apply these constraints, see \"synthetic_data_demo/decision_boundary_demo.py\"\n",
    "    Note: both apply_fairness_constraints and apply_accuracy_constraint cannot be 1 at the same time\n",
    "    sensitive_attrs: [\"s1\", \"s2\", ...], list of sensitive features for which to apply fairness constraint, all of these sensitive features should have a corresponding array in x_control\n",
    "    sensitive_attrs_to_cov_thresh: the covariance threshold that the classifier should achieve (this is only needed when apply_fairness_constraints=1, not needed for the other two constraints)\n",
    "    gamma: controls the loss in accuracy we are willing to incur when using apply_accuracy_constraint and sep_constraint\n",
    "    ----\n",
    "    Outputs:\n",
    "    w: the learned weight vector for the classifier\n",
    "    \"\"\"\n",
    "\n",
    "    assert (\n",
    "        apply_accuracy_constraint == 1 and apply_fairness_constraints == 1\n",
    "    ) == False  # both constraints cannot be applied at the same time\n",
    "\n",
    "    max_iter = (\n",
    "        100000  # maximum number of iterations for the minimization algorithm\n",
    "    )\n",
    "\n",
    "    if apply_fairness_constraints == 0:\n",
    "        constraints = []\n",
    "    else:\n",
    "        constraints = get_constraint_list_cov(\n",
    "            x, y, x_control, sensitive_attrs, sensitive_attrs_to_cov_thresh\n",
    "        )\n",
    "\n",
    "    if (\n",
    "        apply_accuracy_constraint == 0\n",
    "    ):  # its not the reverse problem, just train w with cross cov constraints\n",
    "\n",
    "        f_args = (x, y)\n",
    "        w = minimize(\n",
    "            fun=loss_function,\n",
    "            x0=np.random.rand(x.shape[1],),\n",
    "            args=f_args,\n",
    "            method=\"SLSQP\",\n",
    "            options={\"maxiter\": max_iter},\n",
    "            constraints=constraints,\n",
    "        )\n",
    "\n",
    "    else:\n",
    "\n",
    "        # train on just the loss function\n",
    "        w = minimize(\n",
    "            fun=loss_function,\n",
    "            x0=np.random.rand(x.shape[1],),\n",
    "            args=(x, y),\n",
    "            method=\"SLSQP\",\n",
    "            options={\"maxiter\": max_iter},\n",
    "            constraints=[],\n",
    "        )\n",
    "\n",
    "        old_w = deepcopy(w.x)\n",
    "\n",
    "        def constraint_gamma_all(w, x, y, initial_loss_arr):\n",
    "\n",
    "            gamma_arr = np.ones_like(y) * gamma  # set gamma for everyone\n",
    "            new_loss = loss_function(w, x, y)\n",
    "            old_loss = sum(initial_loss_arr)\n",
    "            return ((1.0 + gamma) * old_loss) - new_loss\n",
    "\n",
    "        def constraint_protected_people(\n",
    "            w, x, y\n",
    "        ):  # dont confuse the protected here with the sensitive feature protected/non-protected values -- protected here means that these points should not be misclassified to negative class\n",
    "            return np.dot(\n",
    "                w, x.T\n",
    "            )  # if this is positive, the constraint is satisfied\n",
    "\n",
    "        def constraint_unprotected_people(w, ind, old_loss, x, y):\n",
    "\n",
    "            new_loss = loss_function(w, np.array([x]), np.array(y))\n",
    "            return ((1.0 + gamma) * old_loss) - new_loss\n",
    "\n",
    "        constraints = []\n",
    "        predicted_labels = np.sign(np.dot(w.x, x.T))\n",
    "        unconstrained_loss_arr = loss_function(w.x, x, y, return_arr=True)\n",
    "\n",
    "        if sep_constraint == True:  # separate gemma for different people\n",
    "            for i in range(0, len(predicted_labels)):\n",
    "                if (\n",
    "                    predicted_labels[i] == 1.0\n",
    "                    and x_control[sensitive_attrs[0]][i] == 1.0\n",
    "                ):  # for now we are assuming just one sensitive attr for reverse constraint, later, extend the code to take into account multiple sensitive attrs\n",
    "                    c = {\n",
    "                        \"type\": \"ineq\",\n",
    "                        \"fun\": constraint_protected_people,\n",
    "                        \"args\": (x[i], y[i]),\n",
    "                    }  # this constraint makes sure that these people stay in the positive class even in the modified classifier\n",
    "                    constraints.append(c)\n",
    "                else:\n",
    "                    c = {\n",
    "                        \"type\": \"ineq\",\n",
    "                        \"fun\": constraint_unprotected_people,\n",
    "                        \"args\": (i, unconstrained_loss_arr[i], x[i], y[i]),\n",
    "                    }\n",
    "                    constraints.append(c)\n",
    "        else:  # same gamma for everyone\n",
    "            c = {\n",
    "                \"type\": \"ineq\",\n",
    "                \"fun\": constraint_gamma_all,\n",
    "                \"args\": (x, y, unconstrained_loss_arr),\n",
    "            }\n",
    "            constraints.append(c)\n",
    "\n",
    "        def cross_cov_abs_optm_func(weight_vec, x_in, x_control_in_arr):\n",
    "            cross_cov = (\n",
    "                x_control_in_arr - np.mean(x_control_in_arr)\n",
    "            ) * np.dot(weight_vec, x_in.T)\n",
    "            return float(abs(sum(cross_cov))) / float(x_in.shape[0])\n",
    "\n",
    "        w = minimize(\n",
    "            fun=cross_cov_abs_optm_func,\n",
    "            x0=old_w,\n",
    "            args=(x, x_control[sensitive_attrs[0]]),\n",
    "            method=\"SLSQP\",\n",
    "            options={\"maxiter\": 100000},\n",
    "            constraints=constraints,\n",
    "        )\n",
    "\n",
    "    try:\n",
    "        assert w.success == True\n",
    "    except:\n",
    "        print(\n",
    "            \"Optimization problem did not converge.. Check the solution returned by the optimizer.\"\n",
    "        )\n",
    "        print(\"Returned solution is:\")\n",
    "        print(w)\n",
    "        raise\n",
    "\n",
    "    return w.x\n",
    "\n",
    "\n",
    "def get_scores(w, x):\n",
    "    scores = 1.0 / (1 + np.exp(-np.dot(x[: w.shape[0]], w)))\n",
    "\n",
    "    return scores\n",
    "\n",
    "\n",
    "def add_intercept(x):\n",
    "    \"\"\" Add intercept to the data before linear classification \"\"\"\n",
    "    m, n = x.shape\n",
    "    intercept = np.ones(m).reshape(m, 1)  # the constant b\n",
    "    return np.concatenate((intercept, x), axis=1)\n",
    "\n",
    "\n",
    "def logistic_loss(w, X, y, return_arr=None):\n",
    "    \"\"\"Computes the logistic loss.\n",
    "    This function is used from scikit-learn source code\n",
    "    Parameters\n",
    "    ----------\n",
    "    w : ndarray, shape (n_features,) or (n_features + 1,)\n",
    "        Coefficient vector.\n",
    "    X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
    "        Training data.\n",
    "    y : ndarray, shape (n_samples,)\n",
    "        Array of labels.\n",
    "    \"\"\"\n",
    "    yz = y * np.dot(X, w)\n",
    "    # Logistic loss is the negative of the log of the logistic function.\n",
    "    if return_arr == True:\n",
    "        out = -(log_logistic(yz))\n",
    "    else:\n",
    "        out = -np.sum(log_logistic(yz))\n",
    "    return out\n",
    "\n",
    "\n",
    "def log_logistic(X):\n",
    "    \"\"\" This function is used from scikit-learn source code. Source link below \"\"\"\n",
    "\n",
    "    \"\"\"Compute the log of the logistic function, ``log(1 / (1 + e ** -x))``.\n",
    "    This implementation is numerically stable because it splits positive and\n",
    "    negative values::\n",
    "        -log(1 + exp(-x_i))     if x_i > 0\n",
    "        x_i - log(1 + exp(x_i)) if x_i <= 0\n",
    "    Parameters\n",
    "    ----------\n",
    "    X: array-like, shape (M, N)\n",
    "        Argument to the logistic function\n",
    "    Returns\n",
    "    -------\n",
    "    out: array, shape (M, N)\n",
    "        Log of the logistic function evaluated at every point in x\n",
    "    Notes\n",
    "    -----\n",
    "    Source code at:\n",
    "    https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/utils/extmath.py\n",
    "    -----\n",
    "    See the blog post describing this implementation:\n",
    "    http://fa.bianp.net/blog/2013/numerical-optimizers-for-logistic-regression/\n",
    "    \"\"\"\n",
    "    if X.ndim > 1:\n",
    "        raise Exception(\"Array of samples cannot be more than 1-D!\")\n",
    "    out = np.empty_like(X)  # same dimensions and data types\n",
    "\n",
    "    idx = X > 0\n",
    "    out[idx] = -np.log(1.0 + np.exp(-X[idx]))\n",
    "    out[~idx] = X[~idx] - np.log(1.0 + np.exp(X[~idx]))\n",
    "    return out\n",
    "\n",
    "\n",
    "def get_constraint_list_cov(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    x_control_train,\n",
    "    sensitive_attrs,\n",
    "    sensitive_attrs_to_cov_thresh,\n",
    "):\n",
    "    \"\"\"\n",
    "    get the list of constraints to be fed to the minimizer\n",
    "    \"\"\"\n",
    "\n",
    "    constraints = []\n",
    "    for attr in sensitive_attrs:\n",
    "\n",
    "        attr_arr = x_control_train[attr]\n",
    "        attr_arr_transformed, index_dict = get_one_hot_encoding(attr_arr)\n",
    "\n",
    "        if index_dict is None:  # binary attribute\n",
    "            thresh = sensitive_attrs_to_cov_thresh[attr]\n",
    "            c = {\n",
    "                \"type\": \"ineq\",\n",
    "                \"fun\": test_sensitive_attr_constraint_cov,\n",
    "                \"args\": (\n",
    "                    x_train,\n",
    "                    y_train,\n",
    "                    attr_arr_transformed,\n",
    "                    thresh,\n",
    "                    False,\n",
    "                ),\n",
    "            }\n",
    "            constraints.append(c)\n",
    "        else:  # otherwise, its a categorical attribute, so we need to set the cov thresh for each value separately\n",
    "            for attr_val, ind in index_dict.items():\n",
    "                attr_name = attr_val\n",
    "                thresh = sensitive_attrs_to_cov_thresh[attr][attr_name]\n",
    "\n",
    "                t = attr_arr_transformed[:, ind]\n",
    "                c = {\n",
    "                    \"type\": \"ineq\",\n",
    "                    \"fun\": test_sensitive_attr_constraint_cov,\n",
    "                    \"args\": (x_train, y_train, t, thresh, False),\n",
    "                }\n",
    "                constraints.append(c)\n",
    "\n",
    "    return constraints\n",
    "\n",
    "\n",
    "def get_one_hot_encoding(in_arr):\n",
    "    \"\"\"\n",
    "        input: 1-D arr with int vals -- if not int vals, will raise an error\n",
    "        output: m (ndarray): one-hot encoded matrix\n",
    "                d (dict): also returns a dictionary original_val -> column in encoded matrix\n",
    "    \"\"\"\n",
    "\n",
    "    for k in in_arr:\n",
    "        if (\n",
    "            str(type(k)) != \"<type 'numpy.float64'>\"\n",
    "            and type(k) != int\n",
    "            and type(k) != np.int64\n",
    "        ):\n",
    "            print(str(type(k)))\n",
    "            print(\"************* ERROR: Input arr does not have integer types\")\n",
    "            return None\n",
    "\n",
    "    in_arr = np.array(in_arr, dtype=int)\n",
    "    assert len(in_arr.shape) == 1  # no column, means it was a 1-D arr\n",
    "    attr_vals_uniq_sorted = sorted(list(set(in_arr)))\n",
    "    num_uniq_vals = len(attr_vals_uniq_sorted)\n",
    "    if (num_uniq_vals == 2) and (\n",
    "        attr_vals_uniq_sorted[0] == 0 and attr_vals_uniq_sorted[1] == 1\n",
    "    ):\n",
    "        return in_arr, None\n",
    "\n",
    "    index_dict = {}  # value to the column number\n",
    "    for i in range(0, len(attr_vals_uniq_sorted)):\n",
    "        val = attr_vals_uniq_sorted[i]\n",
    "        index_dict[val] = i\n",
    "\n",
    "    out_arr = []\n",
    "    for i in range(0, len(in_arr)):\n",
    "        tup = np.zeros(num_uniq_vals)\n",
    "        val = in_arr[i]\n",
    "        ind = index_dict[val]\n",
    "        tup[ind] = 1  # set that value of tuple to 1\n",
    "        out_arr.append(tup)\n",
    "\n",
    "    return np.array(out_arr), index_dict\n",
    "\n",
    "\n",
    "def test_sensitive_attr_constraint_cov(\n",
    "    model, x_arr, y_arr_dist_boundary, x_control, thresh, verbose\n",
    "):\n",
    "\n",
    "    \"\"\"\n",
    "    The covariance is computed b/w the sensitive attr val and the distance from the boundary\n",
    "    If the model is None, we assume that the y_arr_dist_boundary contains the distace from the decision boundary\n",
    "    If the model is not None, we just compute a dot product or model and x_arr\n",
    "    for the case of SVM, we pass the distace from bounday becase the intercept in internalized for the class\n",
    "    and we have compute the distance using the project function\n",
    "    this function will return -1 if the constraint specified by thresh parameter is not satifsified\n",
    "    otherwise it will reutrn +1\n",
    "    if the return value is >=0, then the constraint is satisfied\n",
    "    \"\"\"\n",
    "\n",
    "    assert x_arr.shape[0] == x_control.shape[0]\n",
    "    if (\n",
    "        len(x_control.shape) > 1\n",
    "    ):  # make sure we just have one column in the array\n",
    "        assert x_control.shape[1] == 1\n",
    "\n",
    "    arr = []\n",
    "    if model is None:\n",
    "        arr = y_arr_dist_boundary  # simply the output labels\n",
    "    else:\n",
    "        arr = np.dot(\n",
    "            model, x_arr.T\n",
    "        )  # the product with the weight vector -- the sign of this is the output label\n",
    "\n",
    "    arr = np.array(arr, dtype=np.float64)\n",
    "    cov = np.dot(x_control - np.mean(x_control), arr) / float(len(x_control))\n",
    "    ans = thresh - abs(\n",
    "        cov\n",
    "    )  # will be <0 if the covariance is greater than thresh -- that is, the condition is not satisfied\n",
    "    # ans = thresh - cov # will be <0 if the covariance is greater than thresh -- that is, the condition is not satisfied\n",
    "    if verbose is True:\n",
    "        print(\"Covariance is\", cov)\n",
    "        print(\"Diff is:\", ans)\n",
    "        print()\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"/project/data/adult/processed/train-one-hot.csv\").sample(\n",
    "    2000\n",
    ")\n",
    "test = pd.read_csv(\"/project/data/adult/processed/test-one-hot.csv\").sample(\n",
    "    2000\n",
    ")\n",
    "val = pd.read_csv(\"/project/data/adult/processed/val-one-hot.csv\").sample(6000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train.drop([\"sex\", \"salary\"], axis=1).values\n",
    "y_train = train[\"salary\"].values\n",
    "y_train[y_train == 0] = -1\n",
    "x_control_train = {\"sex\": train[\"sex\"].values}\n",
    "a_train = x_control_train[\"sex\"]\n",
    "\n",
    "x_test = test.drop([\"sex\", \"salary\"], axis=1).values\n",
    "y_test = test[\"salary\"].values\n",
    "y_test[y_test == 0] = -1\n",
    "x_control_test = {\"sex\": test[\"sex\"].values}\n",
    "a_test = x_control_test[\"sex\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_fairness_constraints = 1  # set this flag to one since we want to optimize accuracy subject to fairness constraints\n",
    "apply_accuracy_constraint = 0\n",
    "sep_constraint = 0\n",
    "sensitive_attrs = [\"sex\"]\n",
    "cov_factor = 0.1\n",
    "sensitive_attrs_to_cov_thresh = {\"sex\": cov_factor}\n",
    "loss_function = logistic_loss\n",
    "x_train = add_intercept(\n",
    "    x_train\n",
    ")  # add intercept to X before applying the linear classifier\n",
    "x_test = add_intercept(x_test)\n",
    "gamma = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set type of interevention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimise for accuracy only, i.e., unconstrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_fairness_constraints = 0\n",
    "apply_accuracy_constraint = 0\n",
    "sep_constraint = 0\n",
    "sensitive_attrs_to_cov_thresh = None\n",
    "gamma = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimise for accuracy under fairness constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_fairness_constraints = 1\n",
    "apply_accuracy_constraint = 0\n",
    "sep_constraint = 0\n",
    "sensitive_attrs_to_cov_thresh = {\n",
    "    \"sex\": 0.1\n",
    "}  # controls covariance to protected attribute\n",
    "gamma = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimise for fairness under accuracy constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_fairness_constraints = 0\n",
    "apply_accuracy_constraint = 1\n",
    "sep_constraint = 0\n",
    "sensitive_attrs_to_cov_thresh = None\n",
    "gamma = 0.3  # controls priority of fairness over accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sensitive attribute\n",
    "sensitive_attrs = [\"sex\"]\n",
    "\n",
    "# set loss function\n",
    "loss_function = logistic_loss\n",
    "\n",
    "# add intercept to X before applying the linear classifier\n",
    "x_train = add_intercept(x_train)\n",
    "x_test = add_intercept(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform intervention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "w = train_model(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    x_control_train,\n",
    "    loss_function,\n",
    "    apply_fairness_constraints,\n",
    "    apply_accuracy_constraint,\n",
    "    sep_constraint,\n",
    "    sensitive_attrs,\n",
    "    sensitive_attrs_to_cov_thresh,\n",
    "    gamma,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate demographic parity and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_scores = get_scores(x_test.T, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test[y_test == -1] = 0\n",
    "acc = accuracy(test_scores, test.salary)\n",
    "print(\"Accuracy after fairness intervention =\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp_d = independence_d(test_scores, test.sex)\n",
    "print(\"Demographic parity =\", 1.0 - dp_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "go.Figure(\n",
    "    data=[\n",
    "        go.Bar(\n",
    "            x=[sex],\n",
    "            y=[test_scores[test.sex == sex].mean()],\n",
    "            name=\"Male\" if sex else \"Female\",\n",
    "        )\n",
    "        for sex in range(2)\n",
    "    ]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Python3]",
   "language": "python",
   "name": "conda-env-Python3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
