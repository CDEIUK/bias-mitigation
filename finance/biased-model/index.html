<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta http-equiv="x-ua-compatible" content="ie=edge"/><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"/><style data-href="/cdei-development/styles.c8078ddd3bb7b6940176.css">html{font-family:sans-serif;-ms-text-size-adjust:100%;-webkit-text-size-adjust:100%}body{margin:0;-webkit-font-smoothing:antialiased;-moz-osx-font-smoothing:grayscale}article,aside,details,figcaption,figure,footer,header,main,menu,nav,section,summary{display:block}audio,canvas,progress,video{display:inline-block}audio:not([controls]){display:none;height:0}progress{vertical-align:baseline}[hidden],template{display:none}a{background-color:transparent;-webkit-text-decoration-skip:objects}a:active,a:hover{outline-width:0}abbr[title]{border-bottom:none;text-decoration:underline;-webkit-text-decoration:underline dotted;text-decoration:underline dotted}b,strong{font-weight:inherit;font-weight:bolder}dfn{font-style:italic}h1{font-size:2em;margin:.67em 0}mark{background-color:#ff0;color:#000}small{font-size:80%}sub,sup{font-size:75%;line-height:0;position:relative;vertical-align:baseline}sub{bottom:-.25em}sup{top:-.5em}img{border-style:none}svg:not(:root){overflow:hidden}code,kbd,pre,samp{font-family:monospace,monospace;font-size:1em}figure{margin:1em 40px}hr{box-sizing:content-box;height:0;overflow:visible}button,input,optgroup,select,textarea{font:inherit;margin:0}optgroup{font-weight:700}button,input{overflow:visible}button,select{text-transform:none}[type=reset],[type=submit],button,html [type=button]{-webkit-appearance:button}[type=button]::-moz-focus-inner,[type=reset]::-moz-focus-inner,[type=submit]::-moz-focus-inner,button::-moz-focus-inner{border-style:none;padding:0}[type=button]:-moz-focusring,[type=reset]:-moz-focusring,[type=submit]:-moz-focusring,button:-moz-focusring{outline:1px dotted ButtonText}fieldset{border:1px solid silver;margin:0 2px;padding:.35em .625em .75em}legend{box-sizing:border-box;color:inherit;display:table;max-width:100%;padding:0;white-space:normal}textarea{overflow:auto}[type=checkbox],[type=radio]{box-sizing:border-box;padding:0}[type=number]::-webkit-inner-spin-button,[type=number]::-webkit-outer-spin-button{height:auto}[type=search]{-webkit-appearance:textfield;outline-offset:-2px}[type=search]::-webkit-search-cancel-button,[type=search]::-webkit-search-decoration{-webkit-appearance:none}::-webkit-input-placeholder{color:inherit;opacity:.54}::-webkit-file-upload-button{-webkit-appearance:button;font:inherit}html{font:112.5%/1.45em georgia,serif;box-sizing:border-box;overflow-y:scroll}*,:after,:before{box-sizing:inherit}body{color:rgba(0,0,0,.8);font-family:georgia,serif;font-weight:400;word-wrap:break-word;-webkit-font-kerning:normal;font-kerning:normal;-ms-font-feature-settings:"kern","liga","clig","calt";font-feature-settings:"kern","liga","clig","calt"}img{max-width:100%;padding:0;margin:0 0 1.45rem}h1{font-size:2.25rem}h1,h2{padding:0;margin:0 0 1.45rem;color:inherit;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen,Ubuntu,Cantarell,Fira Sans,Droid Sans,Helvetica Neue,sans-serif;font-weight:700;text-rendering:optimizeLegibility;line-height:1.1}h2{font-size:1.62671rem}h3{font-size:1.38316rem}h3,h4{padding:0;margin:0 0 1.45rem;color:inherit;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen,Ubuntu,Cantarell,Fira Sans,Droid Sans,Helvetica Neue,sans-serif;font-weight:700;text-rendering:optimizeLegibility;line-height:1.1}h4{font-size:1rem}h5{font-size:.85028rem}h5,h6{padding:0;margin:0 0 1.45rem;color:inherit;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen,Ubuntu,Cantarell,Fira Sans,Droid Sans,Helvetica Neue,sans-serif;font-weight:700;text-rendering:optimizeLegibility;line-height:1.1}h6{font-size:.78405rem}hgroup{padding:0;margin:0 0 1.45rem}ol,ul{padding:0;margin:0 0 1.45rem 1.45rem;list-style-position:outside;list-style-image:none}dd,dl,figure,p{padding:0;margin:0 0 1.45rem}pre{margin:0 0 1.45rem;font-size:.85rem;line-height:1.42;background:rgba(0,0,0,.04);border-radius:3px;overflow:auto;word-wrap:normal;padding:1.45rem}table{font-size:1rem;line-height:1.45rem;border-collapse:collapse;width:100%}fieldset,table{padding:0;margin:0 0 1.45rem}blockquote{padding:0;margin:0 1.45rem 1.45rem}form,iframe,noscript{padding:0;margin:0 0 1.45rem}hr{padding:0;margin:0 0 calc(1.45rem - 1px);background:rgba(0,0,0,.2);border:none;height:1px}address{padding:0;margin:0 0 1.45rem}b,dt,strong,th{font-weight:700}li{margin-bottom:.725rem}ol li,ul li{padding-left:0}li>ol,li>ul{margin-left:1.45rem;margin-bottom:.725rem;margin-top:.725rem}blockquote :last-child,li :last-child,p :last-child{margin-bottom:0}li>p{margin-bottom:.725rem}code,kbd,samp{font-size:.85rem;line-height:1.45rem}abbr,abbr[title],acronym{border-bottom:1px dotted rgba(0,0,0,.5);cursor:help}abbr[title]{text-decoration:none}td,th,thead{text-align:left}td,th{border-bottom:1px solid rgba(0,0,0,.12);font-feature-settings:"tnum";-moz-font-feature-settings:"tnum";-ms-font-feature-settings:"tnum";-webkit-font-feature-settings:"tnum";padding:.725rem .96667rem calc(.725rem - 1px)}td:first-child,th:first-child{padding-left:0}td:last-child,th:last-child{padding-right:0}code,tt{background-color:rgba(0,0,0,.04);border-radius:3px;font-family:SFMono-Regular,Consolas,Roboto Mono,Droid Sans Mono,Liberation Mono,Menlo,Courier,monospace;padding:.2em 0}pre code{background:none;line-height:1.42}code:after,code:before,tt:after,tt:before{letter-spacing:-.2em;content:" "}pre code:after,pre code:before,pre tt:after,pre tt:before{content:""}@media only screen and (max-width:480px){html{font-size:100%}}.navigation-module--navigation--_yI7y{display:flex;align-items:center;justify-content:center;width:1024px;max-width:100%;margin:80px 0 40px}.navigation-module--button--N8FV6{position:relative;display:inline-flex;align-items:center;justify-content:center;font-size:1rem;font-weight:700;border-radius:8px;max-width:40%;cursor:pointer;-webkit-appearance:none;-moz-appearance:none;appearance:none;background:#dadada}.navigation-module--button--N8FV6:hover{background:#d0d0d0}.navigation-module--button--N8FV6+.navigation-module--button--N8FV6{margin-left:10px}.navigation-module--button--N8FV6 a{display:flex;padding:8px 16px;text-decoration:none;color:#101010}.navigation-module--button--N8FV6 a,.navigation-module--button--N8FV6 span{text-overflow:ellipsis;white-space:nowrap;overflow:hidden}.navigation-module--iconNext--27P5X{margin-left:8px}.navigation-module--iconPrev--17i6k{margin-right:8px}.toc-module--toc--1rBCH{position:-webkit-sticky;position:sticky;top:50px}.toc-module--toc--1rBCH>ul{list-style-type:none}.toc-module--tocItem--22nlG{color:#9c9c9c;text-decoration:none}.toc-module--tocItem--22nlG:hover{color:#101010}.toc-module--activeTocItem--3VMKc{color:#303030}</style><meta name="generator" content="Gatsby 2.23.3"/><title data-react-helmet="true"></title><style data-styled="" data-styled-version="5.1.1"></style><link rel="icon" href="/cdei-development/favicon-32x32.png?v=80ec5df783a2635a77990defac450c75"/><link rel="manifest" href="/cdei-development/manifest.webmanifest"/><meta name="theme-color" content="#663399"/><link rel="apple-touch-icon" sizes="48x48" href="/cdei-development/icons/icon-48x48.png?v=80ec5df783a2635a77990defac450c75"/><link rel="apple-touch-icon" sizes="72x72" href="/cdei-development/icons/icon-72x72.png?v=80ec5df783a2635a77990defac450c75"/><link rel="apple-touch-icon" sizes="96x96" href="/cdei-development/icons/icon-96x96.png?v=80ec5df783a2635a77990defac450c75"/><link rel="apple-touch-icon" sizes="144x144" href="/cdei-development/icons/icon-144x144.png?v=80ec5df783a2635a77990defac450c75"/><link rel="apple-touch-icon" sizes="192x192" href="/cdei-development/icons/icon-192x192.png?v=80ec5df783a2635a77990defac450c75"/><link rel="apple-touch-icon" sizes="256x256" href="/cdei-development/icons/icon-256x256.png?v=80ec5df783a2635a77990defac450c75"/><link rel="apple-touch-icon" sizes="384x384" href="/cdei-development/icons/icon-384x384.png?v=80ec5df783a2635a77990defac450c75"/><link rel="apple-touch-icon" sizes="512x512" href="/cdei-development/icons/icon-512x512.png?v=80ec5df783a2635a77990defac450c75"/><link as="script" rel="preload" href="/cdei-development/webpack-runtime-1363e7ebfdb3c19bc1c0.js"/><link as="script" rel="preload" href="/cdei-development/framework-81e6052b3504df28bf0b.js"/><link as="script" rel="preload" href="/cdei-development/app-781327009e4245821706.js"/><link as="script" rel="preload" href="/cdei-development/styles-9b6f388623a2ec93d35f.js"/><link as="script" rel="preload" href="/cdei-development/component---src-templates-post-js-d73857b17f89e977d28b.js"/><link as="fetch" rel="preload" href="/cdei-development/page-data/finance/biased-model/page-data.json" crossorigin="anonymous"/><link as="fetch" rel="preload" href="/cdei-development/page-data/app-data.json" crossorigin="anonymous"/></head><body><div id="___gatsby"><div style="outline:none" tabindex="-1" id="gatsby-focus-wrapper"><header style="background:rebeccapurple;margin-bottom:1.45rem"><div style="margin:0 auto;max-width:960px;padding:1.45rem 1.0875rem"><h1 style="margin:0"><a style="color:white;text-decoration:none" href="/cdei-development/">CDEI Bias Exploration</a></h1></div></header><div style="margin:0 auto;max-width:960px;padding:0 1.0875rem 1.45rem"><main><div style="display:grid;grid-template-columns:250px 1fr"><div style="grid-column:1"><div class="toc-module--toc--1rBCH"><h3>Contents</h3><ul style="margin-left:0"><li><a class="toc-module--tocItem--22nlG" href="/cdei-development/finance/intro/">Introduction</a></li><li><a class="toc-module--tocItem--22nlG" href="/cdei-development/finance/data/">The Adult data set</a></li><li><a aria-current="page" class="toc-module--tocItem--22nlG toc-module--activeTocItem--3VMKc" href="/cdei-development/finance/biased-model/">Training a biased model</a></li><li><a class="toc-module--tocItem--22nlG" href="/cdei-development/finance/adversarial-mitigation/">Mitigating Unwanted Biases with Adversarial Learning</a></li><li><a class="toc-module--tocItem--22nlG" href="/cdei-development/finance/intervention_kamiran/">Fairness mitigation via label modification post-processing by Kamiran et al. (2012)</a></li></ul></div></div><div style="grid-column:2;min-width:0"><h1>Training a biased model</h1><p>We start by training a simple model on the data, specifically a shallow, feed-forward neural network. You can see the details <a href="https://mybinder.org/v2/gh/imrehg/cdei-development/master?filepath=notebooks%2Ffinance%2Fbaseline.ipynb" title="Open notebook on Binder">here</a>. To understand the possible sources of bias in our model, it&#x27;s useful to first train a model with no attempted mitigation, and understand the resulting unfairness.</p><h2>Demographic Parity</h2><p>Demographic parity requires that we treat all demographic groups equally. We start by investigating the disparity between the sexes. We do this with box plots of the model scores. A higher score means the model thinks the individual is more likely to be earning more than \$50,000. It&#x27;s clear that there is a major disparity between men and women, with men being awarded systematically higher scores. This model therefore does not achieve demographic parity by some margin.</p><div style="text-align:center"><div aria-busy="true" class=""><svg width="75" height="75" viewBox="0 0 120 30" xmlns="http://www.w3.org/2000/svg" fill="#BF00FF" aria-label="audio-loading"><circle cx="15" cy="15" r="15"><animate attributeName="r" from="15" to="15" begin="0s" dur="0.8s" values="15;9;15" calcMode="linear" repeatCount="indefinite"></animate><animate attributeName="fillOpacity" from="1" to="1" begin="0s" dur="0.8s" values="1;.5;1" calcMode="linear" repeatCount="indefinite"></animate></circle><circle cx="60" cy="15" r="9" attributeName="fillOpacity" from="1" to="0.3"><animate attributeName="r" from="9" to="9" begin="0s" dur="0.8s" values="9;15;9" calcMode="linear" repeatCount="indefinite"></animate><animate attributeName="fillOpacity" from="0.5" to="0.5" begin="0s" dur="0.8s" values=".5;1;.5" calcMode="linear" repeatCount="indefinite"></animate></circle><circle cx="105" cy="15" r="15"><animate attributeName="r" from="15" to="15" begin="0s" dur="0.8s" values="15;9;15" calcMode="linear" repeatCount="indefinite"></animate><animate attributeName="fillOpacity" from="1" to="1" begin="0s" dur="0.8s" values="1;.5;1" calcMode="linear" repeatCount="indefinite"></animate></circle></svg></div></div><p>We do the same thing for race. Again we see that there is a major disparity between races, with white and asian individuals systematically receiving higher scores, and black and native Americans receiving much lower scores.</p><div style="text-align:center"><div aria-busy="true" class=""><svg width="75" height="75" viewBox="0 0 120 30" xmlns="http://www.w3.org/2000/svg" fill="#BF00FF" aria-label="audio-loading"><circle cx="15" cy="15" r="15"><animate attributeName="r" from="15" to="15" begin="0s" dur="0.8s" values="15;9;15" calcMode="linear" repeatCount="indefinite"></animate><animate attributeName="fillOpacity" from="1" to="1" begin="0s" dur="0.8s" values="1;.5;1" calcMode="linear" repeatCount="indefinite"></animate></circle><circle cx="60" cy="15" r="9" attributeName="fillOpacity" from="1" to="0.3"><animate attributeName="r" from="9" to="9" begin="0s" dur="0.8s" values="9;15;9" calcMode="linear" repeatCount="indefinite"></animate><animate attributeName="fillOpacity" from="0.5" to="0.5" begin="0s" dur="0.8s" values=".5;1;.5" calcMode="linear" repeatCount="indefinite"></animate></circle><circle cx="105" cy="15" r="15"><animate attributeName="r" from="15" to="15" begin="0s" dur="0.8s" values="15;9;15" calcMode="linear" repeatCount="indefinite"></animate><animate attributeName="fillOpacity" from="1" to="1" begin="0s" dur="0.8s" values="1;.5;1" calcMode="linear" repeatCount="indefinite"></animate></circle></svg></div></div><h2>Conditional Demographic Parity</h2><p>Conditional demographic parity requires that all demographic groups are treated equally, once certain legitimate risk factors are taken into account. We will take hours worked per week as the legitimate risk factor. We bucket individuals according to how many hours per week they work, and compare the distribution of scores within those buckets first between the two sexes. As we can see in each bucket women receive lower scores than the men. In other words, the model believes that women who work the same number of hours as men are less likely to be high-earners.</p><div style="text-align:center"><div aria-busy="true" class=""><svg width="75" height="75" viewBox="0 0 120 30" xmlns="http://www.w3.org/2000/svg" fill="#BF00FF" aria-label="audio-loading"><circle cx="15" cy="15" r="15"><animate attributeName="r" from="15" to="15" begin="0s" dur="0.8s" values="15;9;15" calcMode="linear" repeatCount="indefinite"></animate><animate attributeName="fillOpacity" from="1" to="1" begin="0s" dur="0.8s" values="1;.5;1" calcMode="linear" repeatCount="indefinite"></animate></circle><circle cx="60" cy="15" r="9" attributeName="fillOpacity" from="1" to="0.3"><animate attributeName="r" from="9" to="9" begin="0s" dur="0.8s" values="9;15;9" calcMode="linear" repeatCount="indefinite"></animate><animate attributeName="fillOpacity" from="0.5" to="0.5" begin="0s" dur="0.8s" values=".5;1;.5" calcMode="linear" repeatCount="indefinite"></animate></circle><circle cx="105" cy="15" r="15"><animate attributeName="r" from="15" to="15" begin="0s" dur="0.8s" values="15;9;15" calcMode="linear" repeatCount="indefinite"></animate><animate attributeName="fillOpacity" from="1" to="1" begin="0s" dur="0.8s" values="1;.5;1" calcMode="linear" repeatCount="indefinite"></animate></circle></svg></div></div><p>We repeat this, grouping individuals by race, and again see that the disadvantaged demographics are less likely to be high-earners, even if they work the same number of hours.</p><div style="text-align:center"><div aria-busy="true" class=""><svg width="75" height="75" viewBox="0 0 120 30" xmlns="http://www.w3.org/2000/svg" fill="#BF00FF" aria-label="audio-loading"><circle cx="15" cy="15" r="15"><animate attributeName="r" from="15" to="15" begin="0s" dur="0.8s" values="15;9;15" calcMode="linear" repeatCount="indefinite"></animate><animate attributeName="fillOpacity" from="1" to="1" begin="0s" dur="0.8s" values="1;.5;1" calcMode="linear" repeatCount="indefinite"></animate></circle><circle cx="60" cy="15" r="9" attributeName="fillOpacity" from="1" to="0.3"><animate attributeName="r" from="9" to="9" begin="0s" dur="0.8s" values="9;15;9" calcMode="linear" repeatCount="indefinite"></animate><animate attributeName="fillOpacity" from="0.5" to="0.5" begin="0s" dur="0.8s" values=".5;1;.5" calcMode="linear" repeatCount="indefinite"></animate></circle><circle cx="105" cy="15" r="15"><animate attributeName="r" from="15" to="15" begin="0s" dur="0.8s" values="15;9;15" calcMode="linear" repeatCount="indefinite"></animate><animate attributeName="fillOpacity" from="1" to="1" begin="0s" dur="0.8s" values="1;.5;1" calcMode="linear" repeatCount="indefinite"></animate></circle></svg></div></div><h2>Equalised Odds</h2><p>Equalised odds requires that we treat groups equally once outcomes are taken into account. That is to say, among individuals who do in fact earn more than $50,000, the model predictions are similar or the same for all demographic groups. Likewise among individuals who don&#x27;t earn more than $50,000, the model treats different groups equally.</p><p>This is equivalent to requiring equal true and false positive rates on different groups, and so we can measure to what extent we are achieving equalised odds by comparing the <a href="https://en.wikipedia.org/wiki/Receiver_operating_characteristic">ROC curves</a>. Doing this first for the two sexes, we see that there is a large disparity, but it is actually women who receive better classification accuracy, that is the false positive rate is lower and the true positive rate is higher.</p><div style="text-align:center"><div aria-busy="true" class=""><svg width="75" height="75" viewBox="0 0 120 30" xmlns="http://www.w3.org/2000/svg" fill="#BF00FF" aria-label="audio-loading"><circle cx="15" cy="15" r="15"><animate attributeName="r" from="15" to="15" begin="0s" dur="0.8s" values="15;9;15" calcMode="linear" repeatCount="indefinite"></animate><animate attributeName="fillOpacity" from="1" to="1" begin="0s" dur="0.8s" values="1;.5;1" calcMode="linear" repeatCount="indefinite"></animate></circle><circle cx="60" cy="15" r="9" attributeName="fillOpacity" from="1" to="0.3"><animate attributeName="r" from="9" to="9" begin="0s" dur="0.8s" values="9;15;9" calcMode="linear" repeatCount="indefinite"></animate><animate attributeName="fillOpacity" from="0.5" to="0.5" begin="0s" dur="0.8s" values=".5;1;.5" calcMode="linear" repeatCount="indefinite"></animate></circle><circle cx="105" cy="15" r="15"><animate attributeName="r" from="15" to="15" begin="0s" dur="0.8s" values="15;9;15" calcMode="linear" repeatCount="indefinite"></animate><animate attributeName="fillOpacity" from="1" to="1" begin="0s" dur="0.8s" values="1;.5;1" calcMode="linear" repeatCount="indefinite"></animate></circle></svg></div></div><p>We see a similar picture when comparing the different races. The races other than white and black aren&#x27;t very well represented in the data, which means the ROC curves are not very smooth and hence not very useful. But comparing just white and black we see that the true positive rate is higher and the false positive rate lower on average for black individuals.</p><div style="text-align:center"><div aria-busy="true" class=""><svg width="75" height="75" viewBox="0 0 120 30" xmlns="http://www.w3.org/2000/svg" fill="#BF00FF" aria-label="audio-loading"><circle cx="15" cy="15" r="15"><animate attributeName="r" from="15" to="15" begin="0s" dur="0.8s" values="15;9;15" calcMode="linear" repeatCount="indefinite"></animate><animate attributeName="fillOpacity" from="1" to="1" begin="0s" dur="0.8s" values="1;.5;1" calcMode="linear" repeatCount="indefinite"></animate></circle><circle cx="60" cy="15" r="9" attributeName="fillOpacity" from="1" to="0.3"><animate attributeName="r" from="9" to="9" begin="0s" dur="0.8s" values="9;15;9" calcMode="linear" repeatCount="indefinite"></animate><animate attributeName="fillOpacity" from="0.5" to="0.5" begin="0s" dur="0.8s" values=".5;1;.5" calcMode="linear" repeatCount="indefinite"></animate></circle><circle cx="105" cy="15" r="15"><animate attributeName="r" from="15" to="15" begin="0s" dur="0.8s" values="15;9;15" calcMode="linear" repeatCount="indefinite"></animate><animate attributeName="fillOpacity" from="1" to="1" begin="0s" dur="0.8s" values="1;.5;1" calcMode="linear" repeatCount="indefinite"></animate></circle></svg></div></div><p>These observations are in contrast to the previously observed demographic disparities. We observed for example that women systematically are given lower scores, which we could argue disadvantages women. On the other hand, we have just observed that classification accuracy is better for women, they are less likely to be misclassified, which we could argue <em>advantages</em> women. Specifically, the higher classification accuracy means they are less likely to be marketted or granted a credit card or a loan which they cannot afford, and hence less likely to default on that credit card or loan and have to deal with the resulting negative consequences. This observation once again underlines the need for understanding the context of the problem, and making a careful determination of what unfairness actually means.</p><div class="navigation-module--navigation--_yI7y"><span class="navigation-module--button--N8FV6"><a href="/cdei-development/finance/data/"><span class="navigation-module--iconPrev--17i6k">←</span><span>The Adult data set</span></a></span><span class="navigation-module--button--N8FV6"><a href="/cdei-development/finance/adversarial-mitigation/"><span>Mitigating Unwanted Biases with Adversarial Learning</span><span class="navigation-module--iconNext--27P5X">→</span></a></span></div></div></div></main></div></div><div id="gatsby-announcer" style="position:absolute;top:0;width:1px;height:1px;padding:0;overflow:hidden;clip:rect(0, 0, 0, 0);white-space:nowrap;border:0" aria-live="assertive" aria-atomic="true"></div></div><script id="gatsby-script-loader">/*<![CDATA[*/window.pagePath="/finance/biased-model/";/*]]>*/</script><script id="gatsby-chunk-mapping">/*<![CDATA[*/window.___chunkMapping={"app":["/app-781327009e4245821706.js"],"component---src-pages-404-js":["/component---src-pages-404-js-2f0ea9db5c834d33474d.js"],"component---src-pages-index-js":["/component---src-pages-index-js-6c22d9a7c87430a0839a.js"],"component---src-templates-post-js":["/component---src-templates-post-js-d73857b17f89e977d28b.js"]};/*]]>*/</script><script src="/cdei-development/component---src-templates-post-js-d73857b17f89e977d28b.js" async=""></script><script src="/cdei-development/styles-9b6f388623a2ec93d35f.js" async=""></script><script src="/cdei-development/app-781327009e4245821706.js" async=""></script><script src="/cdei-development/framework-81e6052b3504df28bf0b.js" async=""></script><script src="/cdei-development/webpack-runtime-1363e7ebfdb3c19bc1c0.js" async=""></script></body></html>